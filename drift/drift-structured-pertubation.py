"""
Shesha Drift - Structured Perturbation Analysis - Quantization & LoRA

Measures representation similarity under structured perturbations:
1. Quantization (FP16 -> INT8 -> INT4)
2. LoRA adapter insertion (varying ranks)

Based on drift-base-instruct.py but replaces base-vs-instruct
comparison with within-model structured perturbations.
"""

import os
import gc
import warnings
import shutil
import hashlib
import numpy as np
import pandas as pd
import torch
from scipy.spatial.distance import pdist
from scipy.spatial import procrustes
from scipy.stats import spearmanr, pearsonr
from scipy.linalg import orthogonal_procrustes
from sklearn.metrics.pairwise import rbf_kernel
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from pathlib import Path

# Authenticate with Hugging Face
from huggingface_hub import login
token = os.environ.get("HF_TOKEN")
if token:
    login(token)
else:
    print("Set HF_TOKEN environment variable")

warnings.filterwarnings("ignore")

# Check for bitsandbytes (quantization)
try:
    import bitsandbytes as bnb
    from transformers import BitsAndBytesConfig
    HAS_BITSANDBYTES = True
    print(">>> bitsandbytes available")
except ImportError:
    HAS_BITSANDBYTES = False
    print(">>> bitsandbytes NOT available - quantization experiments will be skipped")

# Check for peft (LoRA)
try:
    from peft import get_peft_model, LoraConfig, TaskType
    HAS_PEFT = True
    print(">>> peft available")
except ImportError:
    HAS_PEFT = False
    print(">>> peft NOT available - LoRA experiments will be skipped")

# Known causal LM model types for padding side detection
CAUSAL_TYPES = {
    "llama", "mistral", "falcon", "gpt2", "gpt_neo", "gpt_neox", "bloom", "opt",
    "qwen", "qwen2", "gemma", "gemma2", "phi", "stablelm", "mpt", "pythia",
    "tinyllama", "smollm", "starcoder", "codegen", "cohere", "dbrx"
}


def stable_seed(s: str, base: int = 0) -> int:
    """Generate a stable seed from a string, deterministic across Python runs."""
    h = hashlib.sha256(s.encode("utf-8")).digest()
    return (int.from_bytes(h[:8], "little") + base) % (2**32)


# =============================================================================
# RSATOOLBOX CHECK
# =============================================================================
try:
    import rsatoolbox
    from rsatoolbox.rdm.compare import compare as rsatoolbox_compare
    HAS_RSATOOLBOX = True
    print(">>> rsatoolbox available")
except ImportError:
    HAS_RSATOOLBOX = False
    print(">>> rsatoolbox not available")

# =============================================================================
# CONFIG
# =============================================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RANDOM_STATE = 320

np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)
if DEVICE == "cuda":
    torch.cuda.manual_seed_all(RANDOM_STATE)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    DTYPE = torch.bfloat16
    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
    BATCH_SIZE = 256 if total_mem >= 70 else 64 if total_mem >= 35 else 32
    print(f"GPU: {total_mem:.1f}GB, Batch: {BATCH_SIZE}")
else:
    DTYPE = torch.float32
    BATCH_SIZE = 4

OUTPUT_DIR = Path("./shesha-drift")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
print(f"Output directory: {OUTPUT_DIR}")

# Layer to extract embeddings from (-1 = last)
LAYER = -1

# Bootstrap settings
N_BOOTSTRAP = 100

# Embedding settings
MAX_LENGTH = 256

# Quantization configs
QUANT_CONFIGS = [
    {"name": "fp16", "bits": None},
    {"name": "int8", "bits": 8},
    {"name": "int4_nf4", "bits": 4},
]

# LoRA configs - varying rank
LORA_RANKS = [1, 2, 4, 8, 16, 32, 64]
LORA_INIT_SCALE = 0.01  # Fixed init scale for rank comparison

# LoRA configs - varying init scale at fixed rank
LORA_INIT_SCALES = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]
LORA_FIXED_RANK = 8

# Gaussian noise levels (as fraction of parameter std)
GAUSSIAN_NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5]


# =============================================================================
# PROMPT SETS (Generated by LLM)
# =============================================================================
FACTUAL = [
    "The Earth orbits around the Sun.",
    "Water freezes at zero degrees Celsius.",
    "The human body has 206 bones.",
    "Light travels faster than sound.",
    "The Pacific Ocean is the largest ocean.",
    "Diamonds are made of carbon.",
    "The Moon causes tides on Earth.",
    "Oxygen is essential for human survival.",
    "The Great Wall of China spans thousands of miles.",
    "DNA contains genetic information.",
    "The Sahara is the largest hot desert.",
    "Mount Everest is the tallest mountain.",
    "The Amazon is the largest rainforest.",
    "Gold is a chemical element.",
    "The heart pumps blood through the body.",
    "Photosynthesis converts sunlight to energy.",
    "The brain controls the nervous system.",
    "Iron is magnetic.",
    "The Nile is the longest river.",
    "Gravity pulls objects toward Earth.",
    "The atmosphere protects Earth from meteors.",
    "Cells are the basic units of life.",
    "The speed of light is constant.",
    "Earthquakes occur along fault lines.",
    "The human eye can see millions of colors.",
    "Volcanoes release molten rock.",
    "The Earth has one natural satellite.",
    "Sound cannot travel through a vacuum.",
    "The periodic table organizes chemical elements.",
    "Hurricanes form over warm ocean water.",
    "The equator divides Earth into hemispheres.",
    "Mammals are warm-blooded animals.",
    "The ozone layer blocks ultraviolet radiation.",
    "Fossils preserve evidence of ancient life.",
    "The core of the Earth is extremely hot.",
    "Tectonic plates move slowly over time.",
    "The solar system has eight planets.",
    "Chlorophyll makes plants green.",
    "The human skeleton provides structure.",
    "Electricity flows through conductors.",
    "The Atlantic Ocean separates continents.",
    "Bacteria are single-celled organisms.",
    "The Sun is a star.",
    "Rainbows form from light refraction.",
    "The liver detoxifies the blood.",
    "Coral reefs support marine biodiversity.",
    "The Arctic is colder than the Antarctic.",
    "Muscles enable body movement.",
    "The universe is constantly expanding.",
    "Glaciers are made of compressed snow.",
]

DESCRIPTIVE = [
    "The old house stood quietly at the end of the street.",
    "Waves crashed against the rocky shoreline.",
    "The garden was filled with colorful flowers.",
    "A gentle breeze rustled through the leaves.",
    "The city lights sparkled in the distance.",
    "Snow covered the mountain peaks.",
    "The library was silent except for turning pages.",
    "Steam rose from the hot cup of coffee.",
    "The sunset painted the sky orange and pink.",
    "Children played happily in the park.",
    "The ancient castle overlooked the valley.",
    "Rain tapped softly against the window.",
    "The market was bustling with activity.",
    "A lone bird sang in the morning.",
    "The forest was dark and mysterious.",
    "Smoke curled up from the chimney.",
    "The river flowed peacefully through the meadow.",
    "Autumn leaves covered the ground.",
    "The bakery smelled of fresh bread.",
    "Stars filled the clear night sky.",
    "The dog slept peacefully by the fire.",
    "Fog rolled in from the sea.",
    "The train station was crowded with travelers.",
    "Sunlight streamed through the curtains.",
    "The beach stretched for miles.",
    "A cat watched birds from the window.",
    "The clock tower chimed at noon.",
    "Candles flickered in the dark room.",
    "The vineyard covered the hillside.",
    "Thunder rumbled in the distance.",
    "The cafe was warm and inviting.",
    "Shadows lengthened as evening approached.",
    "The fountain splashed in the plaza.",
    "Ice crystals formed on the glass.",
    "The museum displayed ancient artifacts.",
    "Wildflowers bloomed along the path.",
    "The harbor was full of fishing boats.",
    "Morning dew glistened on the grass.",
    "The kitchen was filled with delicious aromas.",
    "Clouds drifted lazily across the sky.",
    "The bookshop was cozy and cluttered.",
    "Fireflies glowed in the summer night.",
    "The bridge spanned the wide river.",
    "Frost covered the windowpane.",
    "The orchard was heavy with ripe fruit.",
    "Waves lapped gently at the shore.",
    "The tower rose above the skyline.",
    "Petals fell from the cherry blossoms.",
    "The lantern cast a warm glow.",
    "Mountains framed the distant horizon.",
]

INSTRUCTIONS = [
    "Explain how photosynthesis works in plants.",
    "Describe the process of making bread from scratch.",
    "List the steps to change a car tire.",
    "Explain the water cycle in simple terms.",
    "Describe how to perform CPR correctly.",
    "Explain the difference between weather and climate.",
    "List the ingredients needed for a basic pasta sauce.",
    "Describe how earthquakes are measured.",
    "Explain the concept of supply and demand.",
    "Describe the steps to plant a vegetable garden.",
    "Explain how vaccines work in the body.",
    "List the major causes of air pollution.",
    "Describe the process of recycling plastic.",
    "Explain the difference between renewable and nonrenewable energy.",
    "Describe how to write a professional email.",
    "Explain the basic principles of electricity.",
    "List the benefits of regular exercise.",
    "Describe how the human digestive system works.",
    "Explain the concept of compound interest.",
    "Describe the steps to create a budget.",
    "Explain how GPS navigation works.",
    "List the symptoms of common cold versus flu.",
    "Describe the process of photographic development.",
    "Explain the difference between mitosis and meiosis.",
    "Describe how to tie a Windsor knot.",
    "Explain the greenhouse effect.",
    "List the main food groups for a balanced diet.",
    "Describe how sound waves travel.",
    "Explain the concept of natural selection.",
    "Describe the steps to start a small business.",
    "Explain how the immune system fights infection.",
    "List the planets in order from the Sun.",
    "Describe the process of making cheese.",
    "Explain the difference between acids and bases.",
    "Describe how to perform basic first aid.",
    "Explain the concept of inflation in economics.",
    "List the steps to prepare for a job interview.",
    "Describe how rainbows are formed.",
    "Explain the basics of machine learning.",
    "Describe the process of wine fermentation.",
    "Explain how the stock market works.",
    "List the causes and effects of deforestation.",
    "Describe how to maintain good mental health.",
    "Explain the theory of relativity simply.",
    "Describe the steps to learn a new language.",
    "Explain how batteries store energy.",
    "List the major events of World War II.",
    "Describe how to write a research paper.",
    "Explain the concept of biodiversity.",
    "Describe the process of cloud formation.",
]

CONVERSATIONAL = [
    "How was your day today?",
    "What do you think about the weather?",
    "Have you seen any good movies lately?",
    "What are your plans for the weekend?",
    "Do you have any book recommendations?",
    "What's your favorite type of music?",
    "Have you tried any new restaurants recently?",
    "What hobbies do you enjoy?",
    "Do you prefer coffee or tea?",
    "What's the best vacation you've ever had?",
    "How do you usually spend your evenings?",
    "What's your favorite season and why?",
    "Do you enjoy cooking at home?",
    "What sports do you follow?",
    "Have you learned anything interesting lately?",
    "What's your morning routine like?",
    "Do you prefer cities or countryside?",
    "What's the last thing that made you laugh?",
    "Do you have any pets?",
    "What's your favorite holiday tradition?",
    "How do you stay motivated?",
    "What's something you're looking forward to?",
    "Do you enjoy gardening?",
    "What's your favorite childhood memory?",
    "How do you like to relax after work?",
    "What's the best advice you've received?",
    "Do you prefer reading or watching movies?",
    "What's your favorite type of cuisine?",
    "Have you picked up any new skills recently?",
    "What's your ideal weekend activity?",
    "Do you enjoy outdoor activities?",
    "What's the most interesting place you've visited?",
    "How do you handle stress?",
    "What's your favorite thing about your job?",
    "Do you prefer mornings or evenings?",
    "What's something you'd like to learn?",
    "Have you attended any concerts lately?",
    "What's your favorite way to exercise?",
    "Do you enjoy board games or puzzles?",
    "What's your go-to comfort food?",
    "How do you stay organized?",
    "What's the best gift you've ever received?",
    "Do you have a favorite local spot?",
    "What's something that inspires you?",
    "How do you like to celebrate birthdays?",
    "What's your favorite thing to do on a rainy day?",
    "Do you enjoy traveling alone or with others?",
    "What's a skill you wish you had?",
    "How do you unwind before bed?",
    "What's something you're grateful for today?",
]

PROMPT_SETS = {
    'factual': FACTUAL,
    'descriptive': DESCRIPTIVE,
    'instructions': INSTRUCTIONS,
    'conversational': CONVERSATIONAL,
}


# =============================================================================
# UTILS
# =============================================================================
def clear_mem():
    gc.collect()
    if DEVICE == "cuda":
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def force_delete_model(model_id):
    """Delete model from HF cache with exact prefix matching."""
    try:
        cache_root = "/root/.cache/huggingface/hub"
        if not os.path.exists(cache_root):
            return

        safe_name = model_id.replace("/", "--")
        patterns = [
            f"models--{safe_name}",
            f"sentence-transformers--{safe_name}",
        ]

        for item in os.listdir(cache_root):
            for pattern in patterns:
                if item == pattern or item.startswith(pattern + "."):
                    path = os.path.join(cache_root, item)
                    if os.path.isdir(path):
                        shutil.rmtree(path)
                        print(f"   [Cleanup] Removed: {item}")
    except Exception as e:
        print(f"   [Cleanup Error] {model_id}: {e}")


def mem_info():
    if DEVICE == "cuda":
        return f"{torch.cuda.memory_allocated()/1e9:.1f}GB"
    return "CPU"


def save_results_incremental(all_results, results_file):
    pd.DataFrame(all_results).to_csv(results_file, index=False)
    print(f"   [SAVED] {len(all_results)} rows")

# =============================================================================
# L2 NORMALIZATION
# =============================================================================
def l2_normalize(X):
    """L2 normalize embeddings to unit norm."""
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    norms = np.maximum(norms, 1e-9)
    return X / norms


# =============================================================================
# METRICS
# =============================================================================
def cka_linear(X, Y):
    """
    Simple linear CKA (not debiased) for sanity checks.
    More numerically stable for self-similarity tests.
    """
    X = X - X.mean(axis=0, keepdims=True)
    Y = Y - Y.mean(axis=0, keepdims=True)

    num = np.linalg.norm(X.T @ Y, 'fro') ** 2
    den = np.linalg.norm(X.T @ X, 'fro') * np.linalg.norm(Y.T @ Y, 'fro')
    return float(num / (den + 1e-12))


def cka_debiased(X, Y):
    """Cleaner implementation of debiased CKA."""
    X = np.asarray(X, dtype=np.float64)
    Y = np.asarray(Y, dtype=np.float64)

    # Center the data
    X = X - X.mean(axis=0, keepdims=True)
    Y = Y - Y.mean(axis=0, keepdims=True)

    n = X.shape[0]
    if n < 4:
        # Fallback for tiny samples (standard CKA)
        num = np.linalg.norm(X.T @ Y, 'fro') ** 2
        den = np.linalg.norm(X.T @ X, 'fro') * np.linalg.norm(Y.T @ Y, 'fro')
        return float(num / (den + 1e-12))

    # Center kernel helper
    def center_gram_matrix(G):
        """Center a Gram matrix: H @ G @ H"""
        row_means = G.mean(axis=1, keepdims=True)
        col_means = G.mean(axis=0, keepdims=True)
        grand_mean = G.mean()
        return G - row_means - col_means + grand_mean

    # Compute and center Gram matrices
    K = center_gram_matrix(X @ X.T)
    L = center_gram_matrix(Y @ Y.T)

    # Zero diagonals for debiasing terms
    K_no_diag = K.copy()
    L_no_diag = L.copy()
    np.fill_diagonal(K_no_diag, 0)
    np.fill_diagonal(L_no_diag, 0)

    # Debiased HSIC estimator (Kornblith et al., 2019)
    hsic = (np.sum(K * L)
            + (np.sum(K_no_diag) * np.sum(L_no_diag)) / ((n-1)*(n-2))
            - 2 * np.sum(np.sum(K_no_diag, axis=1) * np.sum(L_no_diag, axis=1)) / (n-2)
           ) / (n * (n-3))

    # Self-HSIC for normalization
    hsic_xx = (np.sum(K * K)
               + np.sum(K_no_diag)**2 / ((n-1)*(n-2))
               - 2 * np.sum(np.sum(K_no_diag, axis=1)**2) / (n-2)
              ) / (n * (n-3))

    hsic_yy = (np.sum(L * L)
               + np.sum(L_no_diag)**2 / ((n-1)*(n-2))
               - 2 * np.sum(np.sum(L_no_diag, axis=1)**2) / (n-2)
              ) / (n * (n-3))

    if hsic_xx <= 0 or hsic_yy <= 0:
        return 0.0

    return hsic / np.sqrt(hsic_xx * hsic_yy)


def rdm_spearman(X, Y):
    """RDM similarity via Spearman correlation of pairwise cosine distances."""
    rx = pdist(X, 'cosine')
    ry = pdist(Y, 'cosine')
    rho = spearmanr(rx, ry).correlation
    return float(rho) if np.isfinite(rho) else 0.0


def rdm_pearson(X, Y):
    """RDM similarity via Pearson correlation of pairwise cosine distances."""
    rx = pdist(X, 'cosine')
    ry = pdist(Y, 'cosine')
    r, _ = pearsonr(rx, ry)
    return float(r) if np.isfinite(r) else 0.0


def wuc_rsa(X, Y):
    """Whitened Unbiased Cosine RSA using rsatoolbox."""
    if not HAS_RSATOOLBOX:
        return np.nan
    try:
        dx = rsatoolbox.data.Dataset(X.astype(np.float64))
        dy = rsatoolbox.data.Dataset(Y.astype(np.float64))
        rx = rsatoolbox.rdm.calc_rdm(dx, method='euclidean')
        ry = rsatoolbox.rdm.calc_rdm(dy, method='euclidean')
        sim = rsatoolbox_compare(rx, ry, method='cosine_cov')
        return float(sim[0, 0])
    except Exception:
        return np.nan


def procrustes_similarity(X, Y):
    """Procrustes similarity using SciPy's procrustes with robust numerical checks."""
    try:
        # Convert to float64 for better numerical stability
        X = np.asarray(X, dtype=np.float64)
        Y = np.asarray(Y, dtype=np.float64)

        # Check for NaN/Inf values
        if np.any(np.isnan(X)) or np.any(np.isnan(Y)):
            print("      Warning: NaN values in procrustes input")
            return float('nan')
        if np.any(np.isinf(X)) or np.any(np.isinf(Y)):
            print("      Warning: Inf values in procrustes input")
            return float('nan')

        # Check for all-zero or constant columns
        X_std = X.std(axis=0)
        Y_std = Y.std(axis=0)
        if np.any(X_std < 1e-12) or np.any(Y_std < 1e-12):
            print("      Warning: Low variance columns in procrustes input")
            # Add small noise to break degeneracy
            rng = np.random.default_rng(42)
            X = X + rng.normal(0, 1e-8, X.shape)
            Y = Y + rng.normal(0, 1e-8, Y.shape)

        # Center the data
        X_mean = X.mean(axis=0)
        Y_mean = Y.mean(axis=0)
        X_centered = X - X_mean
        Y_centered = Y - Y_mean

        # Check if matrices are degenerate (all points identical after centering)
        X_norm = np.linalg.norm(X_centered, 'fro')
        Y_norm = np.linalg.norm(Y_centered, 'fro')

        if X_norm < 1e-12 or Y_norm < 1e-12:
            print("      Warning: Degenerate matrices in procrustes (norm too small)")
            return float('nan')

        # Scale to unit Frobenius norm
        X_scaled = X_centered / X_norm
        Y_scaled = Y_centered / Y_norm

        # Use try-except for SVD convergence issues
        try:
            # Use orthogonal_procrustes instead of scipy.procrustes for better stability
            R, scale = orthogonal_procrustes(X_scaled, Y_scaled)
        except np.linalg.LinAlgError as e:
            if "SVD did not converge" in str(e):
                print(f"      Warning: SVD did not converge, using fallback")
                # Fallback: compute similarity via correlation of principal angles
                Ux, _, Vx = np.linalg.svd(X_scaled, full_matrices=False)
                Uy, _, Vy = np.linalg.svd(Y_scaled, full_matrices=False)
                cos_angles = np.abs(np.diag(Ux.T @ Uy))
                return float(np.mean(cos_angles))
            else:
                raise

        # Compute distance and convert to similarity
        dist = np.linalg.norm(X_scaled @ R - Y_scaled, 'fro')

        # Theoretical maximum distance for unit-norm matrices is sqrt(2)
        similarity = 1 - dist / np.sqrt(2)

        # Ensure the result is valid
        similarity = np.clip(similarity, 0, 1)

        # Check if result is reasonable
        if not np.isfinite(similarity):
            print(f"      Warning: Non-finite procrustes similarity: {similarity}")
            return float('nan')

        return float(similarity)

    except ValueError as e:
        if "must contain >1 unique points" in str(e):
            print("      Warning: Degenerate case in procrustes (insufficient unique points)")
            return float('nan')
        print(f"      Warning: ValueError in procrustes: {e}")
        return float('nan')
    except np.linalg.LinAlgError as e:
        print(f"      Warning: LinAlgError in procrustes: {e}")
        return float('nan')
    except Exception as e:
        print(f"      Warning: Unexpected error in procrustes: {e}")
        return float('nan')


def sliced_wasserstein(X, Y, n_proj=100, seed=320):
    """Sliced Wasserstein distance with configurable seed."""
    rng = np.random.default_rng(seed)
    dirs = rng.standard_normal((X.shape[1], n_proj))
    dirs /= np.linalg.norm(dirs, axis=0)
    Xp = np.sort(X @ dirs, axis=0)
    Yp = np.sort(Y @ dirs, axis=0)
    return float(np.mean(np.abs(Xp - Yp)))


def mmd_rbf(X, Y, gamma=None, seed=320):
    """Corrected MMD implementation"""
    if gamma is None:
        combined = np.vstack([X, Y])
        if len(combined) > 500:
            rng = np.random.default_rng(seed)
            idx = rng.choice(len(combined), 500, replace=False)
            combined = combined[idx]
        median_dist = np.median(pdist(combined))
        gamma = 1.0 / (max(median_dist, 0.1) ** 2)

    Kxx = rbf_kernel(X, X, gamma)
    Kyy = rbf_kernel(Y, Y, gamma)
    Kxy = rbf_kernel(X, Y, gamma)

    np.fill_diagonal(Kxx, 0)
    np.fill_diagonal(Kyy, 0)

    n, m = len(X), len(Y)

    # CORRECTED: Always use n*m for cross term
    mmd_sq = (Kxx.sum() / (n * (n - 1)) +
              Kyy.sum() / (m * (m - 1)) -
              2 * Kxy.sum() / (n * m))

    return float(np.sqrt(max(0, mmd_sq)))


def subspace_overlap(X, Y, k=10):
    """
    Compute subspace overlap between top-k principal components.
    Returns the mean squared cosine similarity between subspaces.
    """
    try:
        X = np.asarray(X, dtype=np.float64)
        Y = np.asarray(Y, dtype=np.float64)

        # Center the data
        X = X - X.mean(axis=0, keepdims=True)
        Y = Y - Y.mean(axis=0, keepdims=True)

        # Compute SVD
        Ux, Sx, _ = np.linalg.svd(X, full_matrices=False)
        Uy, Sy, _ = np.linalg.svd(Y, full_matrices=False)

        # Take top-k components
        k = min(k, Ux.shape[1], Uy.shape[1])
        Ux_k = Ux[:, :k]
        Uy_k = Uy[:, :k]

        # Compute overlap as mean squared cosine of principal angles
        # This is equivalent to ||Ux_k.T @ Uy_k||_F^2 / k
        overlap_matrix = Ux_k.T @ Uy_k
        overlap = np.sum(overlap_matrix ** 2) / k

        return float(np.clip(overlap, 0, 1))
    except Exception as e:
        print(f"      Warning: subspace_overlap error: {e}")
        return float('nan')


def eigenspectrum_similarity(X, Y):
    """
    Compare eigenspectrum (singular value) distributions.
    Returns cosine similarity between normalized singular value vectors.
    """
    try:
        X = np.asarray(X, dtype=np.float64)
        Y = np.asarray(Y, dtype=np.float64)

        # Center the data
        X = X - X.mean(axis=0, keepdims=True)
        Y = Y - Y.mean(axis=0, keepdims=True)

        # Compute singular values
        Sx = np.linalg.svd(X, compute_uv=False)
        Sy = np.linalg.svd(Y, compute_uv=False)

        # Normalize to unit norm for comparison
        Sx_norm = Sx / (np.linalg.norm(Sx) + 1e-12)
        Sy_norm = Sy / (np.linalg.norm(Sy) + 1e-12)

        # Pad to same length if needed
        max_len = max(len(Sx_norm), len(Sy_norm))
        Sx_padded = np.zeros(max_len)
        Sy_padded = np.zeros(max_len)
        Sx_padded[:len(Sx_norm)] = Sx_norm
        Sy_padded[:len(Sy_norm)] = Sy_norm

        # Cosine similarity
        similarity = np.dot(Sx_padded, Sy_padded)

        return float(np.clip(similarity, 0, 1))
    except Exception as e:
        print(f"      Warning: eigenspectrum_similarity error: {e}")
        return float('nan')


def effective_rank(X, use_log2=False):
    """
    Compute effective rank using spectral ENTROPY of ENERGY (normalized squared singular values).
    Robust measure of effective dimensionality.
    """
    try:
        X = np.asarray(X, dtype=np.float64)
        X_centered = X - X.mean(axis=0, keepdims=True)

        # SVD
        s = np.linalg.svd(X_centered, compute_uv=False)

        # Use Energy (Eigenvalues), not Magnitude (Singular Values) ---
        # This aligns with PCA variance explained and Participation Ratio
        eigenvalues = s ** 2

        # Filter numerical noise
        ev = eigenvalues[eigenvalues > 1e-12]

        if len(ev) == 0:
            return 0.0

        # Normalize to probability distribution
        p = ev / ev.sum()

        # Entropy
        if use_log2:
            h = -np.sum(p * np.log2(p))
            erank = 2 ** h
        else:
            h = -np.sum(p * np.log(p))
            erank = np.exp(h)

        return float(erank)

    except Exception as e:
        print(f"Warning: effective_rank error: {e}")
        return float('nan')

def effective_rank_ratio(X, Y, use_log2=False):
    """
    Compute the ratio of effective ranks.
    Returns: float in [0, 1]
    1.0 = Identical effective dimensionality
    <1.0 = Rank collapse or expansion
    """
    try:
        r_x = effective_rank(X, use_log2)
        r_y = effective_rank(Y, use_log2)

        if r_x == 0 or r_y == 0:
            return 0.0

        # Ratio is always <= 1.0
        return min(r_x, r_y) / max(r_x, r_y)

    except Exception as e:
        return float('nan')


def eigenspectrum_kl_divergence(X, Y, epsilon=1e-10):
    """
    KL divergence between eigenspectrum distributions (treated as probability distributions).
    Lower values indicate more similar spectra.
    """
    try:
        X = np.asarray(X, dtype=np.float64)
        Y = np.asarray(Y, dtype=np.float64)

        # Center the data
        X = X - X.mean(axis=0, keepdims=True)
        Y = Y - Y.mean(axis=0, keepdims=True)

        # Compute singular values squared (eigenvalues of covariance)
        Sx = np.linalg.svd(X, compute_uv=False) ** 2
        Sy = np.linalg.svd(Y, compute_uv=False) ** 2

        # Normalize to probability distributions
        Px = Sx / (Sx.sum() + epsilon)
        Py = Sy / (Sy.sum() + epsilon)

        # Pad to same length
        max_len = max(len(Px), len(Py))
        Px_padded = np.full(max_len, epsilon)
        Py_padded = np.full(max_len, epsilon)
        Px_padded[:len(Px)] = Px + epsilon
        Py_padded[:len(Py)] = Py + epsilon

        # Renormalize after padding
        Px_padded = Px_padded / Px_padded.sum()
        Py_padded = Py_padded / Py_padded.sum()

        # Symmetric KL divergence (Jensen-Shannon style)
        M = 0.5 * (Px_padded + Py_padded)
        kl_pm = np.sum(Px_padded * np.log(Px_padded / M))
        kl_qm = np.sum(Py_padded * np.log(Py_padded / M))
        js_divergence = 0.5 * (kl_pm + kl_qm)

        return float(js_divergence)
    except Exception as e:
        print(f"      Warning: eigenspectrum_kl_divergence error: {e}")
        return float('nan')


def participation_ratio(X):
    """
    Compute participation ratio (effective dimensionality) of a representation.
    PR = (sum of eigenvalues)^2 / sum of eigenvalues^2
    Ranges from 1 (one dominant dimension) to d (uniform spread).
    """
    try:
        X = np.asarray(X, dtype=np.float64)
        X = X - X.mean(axis=0, keepdims=True)

        # Compute eigenvalues of covariance (singular values squared)
        S = np.linalg.svd(X, compute_uv=False) ** 2

        # Participation ratio
        pr = (S.sum() ** 2) / (np.sum(S ** 2) + 1e-12)

        return float(pr)
    except Exception as e:
        print(f"      Warning: participation_ratio error: {e}")
        return float('nan')


def participation_ratio_similarity(X, Y):
    """
    Compare participation ratios of two representations.
    Returns 1 - |PR_X - PR_Y| / max(PR_X, PR_Y) for normalized comparison.
    """
    try:
        pr_x = participation_ratio(X)
        pr_y = participation_ratio(Y)

        if not np.isfinite(pr_x) or not np.isfinite(pr_y):
            return float('nan')

        max_pr = max(pr_x, pr_y)
        if max_pr < 1e-12:
            return 1.0

        similarity = 1 - abs(pr_x - pr_y) / max_pr
        return float(np.clip(similarity, 0, 1))
    except Exception as e:
        print(f"      Warning: participation_ratio_similarity error: {e}")
        return float('nan')


def compute_metrics(X, Y, seed=320):
    """Compute all representation similarity metrics."""
    try:
        X = X.astype(np.float64)
        Y = Y.astype(np.float64)

        if X.shape[0] < 2 or Y.shape[0] < 2:
            return {metric: float('nan') for metric in [
                'cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes',
                'mmd', 'wasserstein', 'subspace_overlap', 'eigenspectrum_sim',
                'eigenspectrum_js', 'participation_ratio_sim', 'pr_x', 'pr_y'
            ]}

        metrics = {}

        try:
            metrics['cka_debiased'] = cka_debiased(X, Y)
        except Exception as e:
            metrics['cka_debiased'] = float('nan')

        try:
            metrics['rdm_spearman'] = rdm_spearman(X, Y)
        except Exception as e:
            metrics['rdm_spearman'] = float('nan')

        try:
            metrics['rdm_pearson'] = rdm_pearson(X, Y)
        except Exception as e:
            metrics['rdm_pearson'] = float('nan')

        try:
            metrics['procrustes'] = procrustes_similarity(X, Y)
        except Exception as e:
            metrics['procrustes'] = float('nan')

        try:
            metrics['mmd'] = mmd_rbf(X, Y, seed=seed)
        except Exception as e:
            metrics['mmd'] = float('nan')

        try:
            metrics['wasserstein'] = sliced_wasserstein(X, Y, seed=seed)
        except Exception as e:
            metrics['wasserstein'] = float('nan')

        # New metrics: subspace overlap
        try:
            metrics['subspace_overlap_k5'] = subspace_overlap(X, Y, k=5)
            metrics['subspace_overlap_k10'] = subspace_overlap(X, Y, k=10)
            metrics['subspace_overlap_k20'] = subspace_overlap(X, Y, k=20)
        except Exception as e:
            metrics['subspace_overlap_k5'] = float('nan')
            metrics['subspace_overlap_k10'] = float('nan')
            metrics['subspace_overlap_k20'] = float('nan')

        # New metrics: eigenspectrum
        try:
            metrics['eigenspectrum_sim'] = eigenspectrum_similarity(X, Y)
        except Exception as e:
            metrics['eigenspectrum_sim'] = float('nan')

        try:
            metrics['effective_rank'] = effective_rank(X)
        except Exception as e:
            metrics['effective_rank'] = float('nan')

            metrics['effective_rank_ratio'] = effective_rank_ratio(X, Y)
        except Exception as e:
            metrics['effective_rank_ratio'] = float('nan')

        try:
            metrics['eigenspectrum_js'] = eigenspectrum_kl_divergence(X, Y)
        except Exception as e:
            metrics['eigenspectrum_js'] = float('nan')

        # New metrics: participation ratio
        try:
            metrics['pr_x'] = participation_ratio(X)
            metrics['pr_y'] = participation_ratio(Y)
            metrics['participation_ratio_sim'] = participation_ratio_similarity(X, Y)
        except Exception as e:
            metrics['pr_x'] = float('nan')
            metrics['pr_y'] = float('nan')
            metrics['participation_ratio_sim'] = float('nan')

        if HAS_RSATOOLBOX:
            try:
                metrics['wuc'] = wuc_rsa(X, Y)
            except Exception as e:
                metrics['wuc'] = float('nan')

        return metrics

    except Exception as e:
        print(f"      Error in compute_metrics: {e}")
        return {metric: float('nan') for metric in [
            'cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes',
            'mmd', 'wasserstein', 'subspace_overlap_k5', 'subspace_overlap_k10',
            'subspace_overlap_k20', 'eigenspectrum_sim', 'eigenspectrum_js',
            'participation_ratio_sim', 'pr_x', 'pr_y'
        ]}


def compute_metrics_with_bootstrap(X, Y, n_bootstrap=None, ci=0.95, seed=None):
    """Compute metrics with bootstrap confidence intervals."""
    if n_bootstrap is None:
        n_bootstrap = N_BOOTSTRAP
    if seed is None:
        seed = RANDOM_STATE

    X = X.astype(np.float64)
    Y = Y.astype(np.float64)
    n = len(X)

    point_metrics = compute_metrics(X, Y, seed=seed)
    bootstrap_metrics = ['rdm_spearman', 'cka_debiased', 'procrustes']

    rng = np.random.default_rng(seed)
    bootstrap_samples = {m: [] for m in bootstrap_metrics}

    for _ in range(n_bootstrap):
        idx = rng.choice(n, size=n, replace=True)
        Xb, Yb = X[idx], Y[idx]

        for metric in bootstrap_metrics:
            if metric == 'rdm_spearman':
                val = rdm_spearman(Xb, Yb)
            elif metric == 'cka_debiased':
                val = cka_debiased(Xb, Yb)
            elif metric == 'procrustes':
                val = procrustes_similarity(Xb, Yb)
            else:
                continue
            bootstrap_samples[metric].append(val)

    alpha = (1 - ci) / 2
    result = {}
    for metric, value in point_metrics.items():
        result[metric] = value
        if metric in bootstrap_samples and bootstrap_samples[metric]:
            samples = np.array(bootstrap_samples[metric])
            result[f"{metric}_ci_lo"] = float(np.percentile(samples, alpha * 100))
            result[f"{metric}_ci_hi"] = float(np.percentile(samples, (1 - alpha) * 100))

    return result


# =============================================================================
# MODEL LOADING
# =============================================================================
def load_model_fp16(model_name):
    """Load model in FP16/BF16 (baseline)."""
    print(f"   Loading {model_name} [FP16] [{mem_info()}]")

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        use_fast=True,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=DTYPE,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    ).to(DEVICE).eval()

    model_type = getattr(model.config, "model_type", "").lower()
    if model_type in CAUSAL_TYPES:
        tokenizer.padding_side = "left"
    else:
        tokenizer.padding_side = "right"

    return model, tokenizer


def load_model_quantized(model_name, bits=8):
    """Load model with INT8 or INT4 quantization."""
    if not HAS_BITSANDBYTES:
        return None, None

    print(f"   Loading {model_name} [INT{bits}] [{mem_info()}]")

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        use_fast=True,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    if bits == 8:
        quant_config = BitsAndBytesConfig(load_in_8bit=True)
    elif bits == 4:
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=DTYPE,
            bnb_4bit_quant_type="nf4"
        )
    else:
        raise ValueError(f"Unsupported bits: {bits}")

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quant_config,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        device_map="auto"
    ).eval()

    model_type = getattr(model.config, "model_type", "").lower()
    if model_type in CAUSAL_TYPES:
        tokenizer.padding_side = "left"
    else:
        tokenizer.padding_side = "right"

    return model, tokenizer


def load_model_with_lora(model_name, lora_rank, lora_alpha, init_scale, seed=320):
    """Load model and apply LoRA adapter with random initialization."""
    if not HAS_PEFT:
        return None, None

    print(f"   Loading {model_name} [LoRA r={lora_rank}, scale={init_scale}] [{mem_info()}]")

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        use_fast=True,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=DTYPE,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    ).to(DEVICE)

    model_type = getattr(model.config, "model_type", "").lower()

    # Target modules by architecture
    if model_type in ["llama", "mistral", "qwen", "qwen2", "gemma", "gemma2"]:
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
    elif model_type in ["gpt2", "gpt_neo", "gpt_neox"]:
        target_modules = ["c_attn", "c_proj"]
    elif model_type in ["bloom"]:
        target_modules = ["query_key_value", "dense"]
    elif model_type in ["falcon"]:
        target_modules = ["query_key_value", "dense"]
    elif model_type in ["opt"]:
        target_modules = ["q_proj", "v_proj", "k_proj", "out_proj"]
    elif model_type in ["phi"]:
        target_modules = ["q_proj", "v_proj", "k_proj", "dense"]
    else:
        target_modules = ["q_proj", "v_proj"]

    try:
        lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=0.0,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )

        model = get_peft_model(model, lora_config)

        # Initialize LoRA weights with controlled random values
        torch.manual_seed(seed)

        with torch.no_grad():
            for name, param in model.named_parameters():
                if "lora_" in name:
                    if "lora_A" in name:
                        fan_in = param.shape[1] if len(param.shape) > 1 else param.shape[0]
                        std = np.sqrt(2.0 / fan_in)
                        param.data = torch.randn_like(param) * std
                    elif "lora_B" in name:
                        param.data = torch.randn_like(param) * init_scale

        model.eval()

    except Exception as e:
        print(f"      LoRA failed: {e}, returning base model")
        return load_model_fp16(model_name)

    if model_type in CAUSAL_TYPES:
        tokenizer.padding_side = "left"
    else:
        tokenizer.padding_side = "right"

    return model, tokenizer


def inject_gaussian_noise(model, alpha, seed=320):
    """
    Inject Gaussian noise into model parameters.
    alpha: noise level as fraction of each parameter's std
    """
    if alpha == 0:
        return

    torch.manual_seed(seed)
    if DEVICE == "cuda":
        torch.cuda.manual_seed_all(seed)

    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad and p.numel() > 1:
                std = float(p.std().item())
                if std > 0:
                    noise = torch.randn_like(p) * (std * alpha)
                    p.add_(noise)


def get_model_state_dict(model):
    """Get a copy of model state dict on CPU."""
    return {k: v.cpu().clone() for k, v in model.state_dict().items()}


def load_model_state_dict(model, state_dict):
    """Load state dict back into model."""
    device_state = {k: v.to(DEVICE) for k, v in state_dict.items()}
    model.load_state_dict(device_state)


# =============================================================================
# EMBEDDING
# =============================================================================
def embed_texts(model, tokenizer, texts):
    """Embed texts using mean pooling of hidden states, L2 normalized."""
    all_vecs = []

    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i:i+BATCH_SIZE]
        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_LENGTH
        ).to(DEVICE)

        with torch.no_grad():
            if DEVICE == "cuda":
                with torch.cuda.amp.autocast(enabled=True, dtype=DTYPE):
                    out = model(**inputs, output_hidden_states=True, return_dict=True)
            else:
                out = model(**inputs, output_hidden_states=True, return_dict=True)

            h = out.hidden_states[LAYER]
            mask = inputs["attention_mask"].unsqueeze(-1)
            vecs = (h * mask).sum(1) / mask.sum(1).clamp(min=1e-9)
            all_vecs.append(vecs.float().cpu().numpy())

        del inputs, out, h, vecs

    embeddings = np.vstack(all_vecs)
    return l2_normalize(embeddings)


def embed_all_prompt_sets(model, tokenizer):
    """Embed all prompt sets."""
    embeddings = {}
    for set_name, prompts in PROMPT_SETS.items():
        emb = embed_texts(model, tokenizer, prompts)
        embeddings[set_name] = emb
        print(f"      {set_name}: {emb.shape}")
    return embeddings


# =============================================================================
# MODELS
# =============================================================================
MODELS = [
    ("HuggingFaceTB/SmolLM-135M", 0.135),
    ("HuggingFaceTB/SmolLM2-135M", 0.135),
    ("HuggingFaceTB/SmolLM-360M", 0.36),
    ("HuggingFaceTB/SmolLM2-360M", 0.36),
    ("Qwen/Qwen2-0.5B", 0.5),
    ("meta-llama/Llama-3.2-1B", 1.0),
    ("TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T", 1.1),
    ("Qwen/Qwen2-1.5B", 1.5),
    ("stabilityai/stablelm-2-1_6b", 1.6),
    ("HuggingFaceTB/SmolLM-1.7B", 1.7),
    ("HuggingFaceTB/SmolLM2-1.7B", 1.7),
    ("google/gemma-2b", 2.0),
    ("google/gemma-2-2b", 2.0),
    ("meta-llama/Llama-3.2-3B", 3.0),
    ("Qwen/Qwen2-7B", 7.0),
    ("mistralai/Mistral-7B-v0.1", 7.0),
]



# =============================================================================
# QUANTIZATION EXPERIMENT
# =============================================================================
def run_quantization_experiment():
    """Compare FP16 vs INT8 vs INT4 representations."""
    if not HAS_BITSANDBYTES:
        print("\n[SKIP] Quantization experiment - bitsandbytes not available")
        return

    results_file = f"{OUTPUT_DIR}/quantization_comparison_results.csv"
    all_results = []
    completed = set()

    if os.path.exists(results_file):
        try:
            df_exist = pd.read_csv(results_file)
            completed = set(df_exist['model'].unique())
            all_results = df_exist.to_dict('records')
            print(f"Resuming quantization... {len(completed)} models done")
        except:
            pass

    print(f"\n{'='*60}")
    print("QUANTIZATION EXPERIMENT: FP16 vs INT8 vs INT4")
    print(f"{'='*60}")

    for model_name, size in MODELS:
        if model_name in completed:
            print(f"\n[Skip] {model_name}")
            continue

        print(f"\n{'='*60}")
        print(f"{model_name} ({size}B)")
        print(f"{'='*60}")

        if DEVICE == "cuda" and size >= 7:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
            if gpu_mem < 35:
                print("   Skipping (GPU too small)")
                continue

        embeddings = {}

        try:
            # Load each quantization level
            for qconfig in QUANT_CONFIGS:
                qname = qconfig["name"]
                bits = qconfig["bits"]

                if bits is None:
                    model, tokenizer = load_model_fp16(model_name)
                else:
                    model, tokenizer = load_model_quantized(model_name, bits)

                if model is None:
                    continue

                emb = embed_all_prompt_sets(model, tokenizer)
                embeddings[qname] = emb

                del model, tokenizer
                clear_mem()

            if "fp16" not in embeddings:
                print("   No FP16 baseline, skipping")
                continue

            # Compute metrics for each quantization vs FP16
            fp16_emb = embeddings["fp16"]
            pair_seed = stable_seed(f"quant_{model_name}", RANDOM_STATE)

            for qname, emb in embeddings.items():
                record = {
                    "model": model_name,
                    "size": size,
                    "perturbation_type": "quantization",
                    "quantization": qname,
                    "layer": LAYER,
                    "pair_seed": pair_seed,
                }

                for i_set, set_name in enumerate(PROMPT_SETS):
                    X = fp16_emb[set_name]
                    Y = emb[set_name]

                    bootstrap_seed = pair_seed + 17 + i_set
                    metrics = compute_metrics_with_bootstrap(X, Y, seed=bootstrap_seed)

                    for metric_name, value in metrics.items():
                        record[f"{metric_name}_{set_name}"] = value
                        if metric_name in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'wuc']:
                            record[f"dissim_{metric_name}_{set_name}"] = 1 - value if np.isfinite(value) else np.nan

                all_results.append(record)

                if qname != "fp16":
                    rdm_s = record.get('rdm_spearman_factual', 0)
                    cka = record.get('cka_debiased_factual', 0)
                    print(f"   {qname}: rdm_spearman={rdm_s:.4f}, cka={cka:.4f}")

            completed.add(model_name)
            pd.DataFrame(all_results).to_csv(results_file, index=False)
            print(f"   Saved ({len(all_results)} rows)")

        except Exception as e:
            print(f"   [Error] {e}")
            import traceback
            traceback.print_exc()

        finally:
            del embeddings
            clear_mem()
            force_delete_model(model_name)

    print(f"\nQuantization experiment complete: {results_file}")

    if all_results:
        df = pd.DataFrame(all_results)
        print(f"\nTotal rows: {len(df)}")

        print("\n--- MEAN SIMILARITY BY QUANTIZATION ---")
        for qname in ["int8", "int4_nf4"]:
            subset = df[df['quantization'] == qname]
            if len(subset) > 0:
                cols = [c for c in subset.columns if c.startswith('rdm_spearman_') and not c.endswith('_ci_lo') and not c.endswith('_ci_hi')]
                if cols:
                    print(f"  {qname}: rdm_spearman={subset[cols].mean().mean():.4f}")


# =============================================================================
# LORA EXPERIMENT
# =============================================================================
def run_lora_experiment():
    """Compare base model vs models with LoRA adapters."""
    if not HAS_PEFT:
        print("\n[SKIP] LoRA experiment - peft not available")
        return

    results_file = f"{OUTPUT_DIR}/lora_comparison_results.csv"
    all_results = []
    completed = set()

    if os.path.exists(results_file):
        try:
            df_exist = pd.read_csv(results_file)
            for _, row in df_exist.iterrows():
                key = f"{row['model']}_r{row['lora_rank']}_s{row['init_scale']}"
                completed.add(key)
            all_results = df_exist.to_dict('records')
            print(f"Resuming LoRA... {len(completed)} configs done")
        except:
            pass

    print(f"\n{'='*60}")
    print("LORA EXPERIMENT: Base vs LoRA Adapters")
    print(f"{'='*60}")

    for model_name, size in MODELS:
        print(f"\n{'='*60}")
        print(f"{model_name} ({size}B)")
        print(f"{'='*60}")

        if DEVICE == "cuda" and size >= 7:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
            if gpu_mem < 35:
                print("   Skipping (GPU too small)")
                continue

        fp16_emb = None

        try:
            # Helper function to load FP16 baseline and optionally save baseline record
            def ensure_fp16_baseline():
                nonlocal fp16_emb, all_results, completed

                if fp16_emb is not None:
                    return  # Already loaded

                model, tokenizer = load_model_fp16(model_name)
                fp16_emb = embed_all_prompt_sets(model, tokenizer)
                del model, tokenizer
                clear_mem()

                # Always save baseline record if not already in results
                baseline_key = f"{model_name}_r0_s0.0"
                if baseline_key not in completed:
                    pair_seed = stable_seed(f"lora_{model_name}_r0", RANDOM_STATE)
                    record = {
                        "model": model_name,
                        "size": size,
                        "perturbation_type": "lora",
                        "lora_rank": 0,
                        "lora_alpha": 0,
                        "init_scale": 0.0,
                        "layer": LAYER,
                        "pair_seed": pair_seed,
                    }

                    # Self-similarity (should be ~1)
                    for i_set, set_name in enumerate(PROMPT_SETS):
                        X = fp16_emb[set_name]
                        bootstrap_seed = pair_seed + 17 + i_set
                        metrics = compute_metrics_with_bootstrap(X, X, seed=bootstrap_seed)
                        for metric_name, value in metrics.items():
                            record[f"{metric_name}_{set_name}"] = value

                    all_results.append(record)
                    completed.add(baseline_key)
                    save_results_incremental(all_results, results_file)
                    print(f"   Baseline (r=0): self-similarity recorded")

            # Check if we need to run any configs for this model
            configs_to_run = []
            for lora_rank in LORA_RANKS:
                config_key = f"{model_name}_r{lora_rank}_s{LORA_INIT_SCALE}"
                if config_key not in completed:
                    configs_to_run.append(("rank", lora_rank, LORA_INIT_SCALE))

            for init_scale in LORA_INIT_SCALES:
                config_key = f"{model_name}_r{LORA_FIXED_RANK}_s{init_scale}"
                if config_key not in completed:
                    configs_to_run.append(("scale", LORA_FIXED_RANK, init_scale))

            # Also check baseline
            baseline_key = f"{model_name}_r0_s0.0"
            if baseline_key not in completed:
                configs_to_run.append(("baseline", 0, 0.0))

            if not configs_to_run:
                print(f"   All configs completed, skipping")
                continue

            # Load FP16 baseline (will also save baseline record if needed)
            ensure_fp16_baseline()

            # Test varying ranks
            for lora_rank in LORA_RANKS:
                lora_alpha = lora_rank * 2
                init_scale = LORA_INIT_SCALE

                config_key = f"{model_name}_r{lora_rank}_s{init_scale}"
                if config_key in completed:
                    continue

                seed = stable_seed(f"lora_{model_name}_r{lora_rank}", RANDOM_STATE)

                model, tokenizer = load_model_with_lora(model_name, lora_rank, lora_alpha, init_scale, seed=seed)
                if model is None:
                    continue

                lora_emb = embed_all_prompt_sets(model, tokenizer)
                del model, tokenizer
                clear_mem()

                record = {
                    "model": model_name,
                    "size": size,
                    "perturbation_type": "lora",
                    "lora_rank": lora_rank,
                    "lora_alpha": lora_alpha,
                    "init_scale": init_scale,
                    "layer": LAYER,
                    "pair_seed": seed,
                }

                for i_set, set_name in enumerate(PROMPT_SETS):
                    X = fp16_emb[set_name]
                    Y = lora_emb[set_name]
                    bootstrap_seed = seed + 17 + i_set
                    metrics = compute_metrics_with_bootstrap(X, Y, seed=bootstrap_seed)

                    for metric_name, value in metrics.items():
                        record[f"{metric_name}_{set_name}"] = value
                        if metric_name in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'wuc']:
                            record[f"dissim_{metric_name}_{set_name}"] = 1 - value if np.isfinite(value) else np.nan

                all_results.append(record)
                completed.add(config_key)
                save_results_incremental(all_results, results_file)

                rdm_s = record.get('rdm_spearman_factual', 0)
                cka = record.get('cka_debiased_factual', 0)
                print(f"   r={lora_rank}: rdm_spearman={rdm_s:.4f}, cka={cka:.4f}")

                del lora_emb
                clear_mem()

            # Test varying init scales at fixed rank
            for init_scale in LORA_INIT_SCALES:
                lora_rank = LORA_FIXED_RANK
                lora_alpha = lora_rank * 2

                config_key = f"{model_name}_r{lora_rank}_s{init_scale}"
                if config_key in completed:
                    continue

                seed = stable_seed(f"lora_{model_name}_r{lora_rank}_s{init_scale}", RANDOM_STATE)

                model, tokenizer = load_model_with_lora(model_name, lora_rank, lora_alpha, init_scale, seed=seed)
                if model is None:
                    continue

                lora_emb = embed_all_prompt_sets(model, tokenizer)
                del model, tokenizer
                clear_mem()

                record = {
                    "model": model_name,
                    "size": size,
                    "perturbation_type": "lora",
                    "lora_rank": lora_rank,
                    "lora_alpha": lora_alpha,
                    "init_scale": init_scale,
                    "layer": LAYER,
                    "pair_seed": seed,
                }

                for i_set, set_name in enumerate(PROMPT_SETS):
                    X = fp16_emb[set_name]
                    Y = lora_emb[set_name]
                    bootstrap_seed = seed + 17 + i_set
                    metrics = compute_metrics_with_bootstrap(X, Y, seed=bootstrap_seed)

                    for metric_name, value in metrics.items():
                        record[f"{metric_name}_{set_name}"] = value
                        if metric_name in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'wuc']:
                            record[f"dissim_{metric_name}_{set_name}"] = 1 - value if np.isfinite(value) else np.nan

                all_results.append(record)
                completed.add(config_key)
                save_results_incremental(all_results, results_file)

                rdm_s = record.get('rdm_spearman_factual', 0)
                cka = record.get('cka_debiased_factual', 0)
                print(f"   r={lora_rank}, scale={init_scale}: rdm_spearman={rdm_s:.4f}, cka={cka:.4f}")

                del lora_emb
                clear_mem()

        except Exception as e:
            print(f"   [Error] {e}")
            import traceback
            traceback.print_exc()

        finally:
            del fp16_emb
            clear_mem()
            force_delete_model(model_name)

    print(f"\nLoRA experiment complete: {results_file}")

    if all_results:
        df = pd.DataFrame(all_results)
        print(f"\nTotal rows: {len(df)}")

        print("\n--- MEAN SIMILARITY BY LORA RANK ---")
        for rank in LORA_RANKS:
            subset = df[(df['lora_rank'] == rank) & (df['init_scale'] == LORA_INIT_SCALE)]
            if len(subset) > 0:
                cols = [c for c in subset.columns if c.startswith('rdm_spearman_') and not c.endswith('_ci_lo') and not c.endswith('_ci_hi')]
                if cols:
                    print(f"  r={rank}: rdm_spearman={subset[cols].mean().mean():.4f}")

# =============================================================================
# GAUSSIAN NOISE EXPERIMENT
# =============================================================================
def run_gaussian_noise_experiment():
    """Compare clean model vs models with Gaussian noise injection."""

    results_file = f"{OUTPUT_DIR}/gaussian_noise_comparison_results.csv"
    all_results = []
    completed = set()

    if os.path.exists(results_file):
        try:
            df_exist = pd.read_csv(results_file)
            for _, row in df_exist.iterrows():
                key = f"{row['model']}_noise{row['noise_level']}"
                completed.add(key)
            all_results = df_exist.to_dict('records')
            print(f"Resuming Gaussian noise... {len(completed)} configs done")
        except:
            pass

    print(f"\n{'='*60}")
    print("GAUSSIAN NOISE EXPERIMENT")
    print(f"{'='*60}")

    for model_name, size in MODELS:
        print(f"\n{'='*60}")
        print(f"{model_name} ({size}B)")
        print(f"{'='*60}")

        if DEVICE == "cuda" and size >= 7:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
            if gpu_mem < 35:
                print("   Skipping (GPU too small)")
                continue

        # Check if we have any noise levels to run
        configs_to_run = [
            alpha for alpha in GAUSSIAN_NOISE_LEVELS
            if f"{model_name}_noise{alpha}" not in completed
        ]

        if not configs_to_run:
            print(f"   All noise levels completed, skipping")
            continue

        fp16_emb = None
        clean_state_dict = None

        try:
            # Load model once
            model, tokenizer = load_model_fp16(model_name)

            # Save clean state dict for restoration
            clean_state_dict = get_model_state_dict(model)

            # Get clean embeddings
            fp16_emb = embed_all_prompt_sets(model, tokenizer)

            # Save baseline record if needed
            baseline_key = f"{model_name}_noise0.0"
            if baseline_key not in completed:
                pair_seed = stable_seed(f"gaussian_{model_name}_0.0", RANDOM_STATE)
                record = {
                    "model": model_name,
                    "size": size,
                    "perturbation_type": "gaussian_noise",
                    "noise_level": 0.0,
                    "layer": LAYER,
                    "pair_seed": pair_seed,
                }

                # Self-similarity
                for i_set, set_name in enumerate(PROMPT_SETS):
                    X = fp16_emb[set_name]
                    bootstrap_seed = pair_seed + 17 + i_set
                    metrics = compute_metrics_with_bootstrap(X, X, seed=bootstrap_seed)
                    for metric_name, value in metrics.items():
                        record[f"{metric_name}_{set_name}"] = value

                all_results.append(record)
                completed.add(baseline_key)
                print(f"   Baseline (noise=0): self-similarity recorded")

            # Test each noise level
            for alpha in GAUSSIAN_NOISE_LEVELS:
                if alpha == 0.0:
                    continue  # Already handled

                config_key = f"{model_name}_noise{alpha}"
                if config_key in completed:
                    continue

                # Restore clean weights
                load_model_state_dict(model, clean_state_dict)

                # Inject noise
                seed = stable_seed(f"gaussian_{model_name}_{alpha}", RANDOM_STATE)
                inject_gaussian_noise(model, alpha, seed=seed)

                # Get noisy embeddings
                noisy_emb = embed_all_prompt_sets(model, tokenizer)

                record = {
                    "model": model_name,
                    "size": size,
                    "perturbation_type": "gaussian_noise",
                    "noise_level": alpha,
                    "layer": LAYER,
                    "pair_seed": seed,
                }

                for i_set, set_name in enumerate(PROMPT_SETS):
                    X = fp16_emb[set_name]
                    Y = noisy_emb[set_name]
                    bootstrap_seed = seed + 17 + i_set
                    metrics = compute_metrics_with_bootstrap(X, Y, seed=bootstrap_seed)

                    for metric_name, value in metrics.items():
                        record[f"{metric_name}_{set_name}"] = value
                        if metric_name in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes',
                                          'wuc', 'subspace_overlap_k5', 'subspace_overlap_k10',
                                          'subspace_overlap_k20', 'eigenspectrum_sim', 'participation_ratio_sim']:
                            record[f"dissim_{metric_name}_{set_name}"] = 1 - value if np.isfinite(value) else np.nan

                all_results.append(record)
                completed.add(config_key)

                rdm_s = record.get('rdm_spearman_factual', 0)
                cka = record.get('cka_debiased_factual', 0)
                print(f"   noise={alpha}: rdm_spearman={rdm_s:.4f}, cka={cka:.4f}")

                del noisy_emb
                clear_mem()

            del model, tokenizer
            pd.DataFrame(all_results).to_csv(results_file, index=False)
            print(f"   Saved ({len(all_results)} rows)")

        except Exception as e:
            print(f"   [Error] {e}")
            import traceback
            traceback.print_exc()

        finally:
            del fp16_emb, clean_state_dict
            clear_mem()
            force_delete_model(model_name)

    print(f"\nGaussian noise experiment complete: {results_file}")

    if all_results:
        df = pd.DataFrame(all_results)
        print(f"\nTotal rows: {len(df)}")

        print("\n--- MEAN SIMILARITY BY NOISE LEVEL ---")
        for alpha in GAUSSIAN_NOISE_LEVELS[1:]:  # Skip 0
            subset = df[df['noise_level'] == alpha]
            if len(subset) > 0:
                cols = [c for c in subset.columns if c.startswith('rdm_spearman_') and not c.endswith('_ci_lo') and not c.endswith('_ci_hi')]
                if cols:
                    print(f"  noise={alpha}: rdm_spearman={subset[cols].mean().mean():.4f}")


# =============================================================================
# MAIN
# =============================================================================
def run_all():
    """Run all three experiments."""
    print("\n" + "="*70)
    print("STRUCTURED PERTURBATION EXPERIMENTS")
    print("="*70)
    print(f"rsatoolbox: {HAS_RSATOOLBOX}")
    print(f"bitsandbytes: {HAS_BITSANDBYTES}")
    print(f"peft: {HAS_PEFT}")
    print(f"Prompts per set: {len(FACTUAL)}")

    # # 1. Gaussian noise experiment
    run_gaussian_noise_experiment()

    # # 2. Quantization experiment
    run_quantization_experiment()

    # 3. LoRA experiment
    run_lora_experiment()

    print("\n" + "="*70)
    print("ALL EXPERIMENTS COMPLETE")
    print("="*70)
    print(f"Results saved to: {OUTPUT_DIR}")


if __name__ == "__main__":
    run_all()
