"""
Shesha Drift - Base vs Instruct Representation Comparison

Cross-model RDM correlation measuring geometric preservation after
instruction tuning. Unlike core Shesha (within-model stability), this
variant assesses whether fine-tuning preserves or reorganizes the
representational geometry.

Key features:
- Load model once per checkpoint with AutoModelForCausalLM fallback
- 50 prompts per category for statistical power
- Chat template applied if tokenizer.chat_template exists
- ADD_GENERATION_PROMPT=False default (avoids assistant prefix bias)
- Robust padding_side detection (is_decoder, CAUSAL_TYPES allowlist)
- L2 normalization before all metrics
- Configurable layer extraction (LAYER parameter)
- ALL stochastic metrics (wasserstein, mmd, bootstrap) use per-pair-set stable seeds
- Sanity checks: self-similarity, permutation null (with sign flip), cross-set null
- Proper cleanup on model reload to avoid memory leaks
- Records all config + seeds + reload status in CSV
"""

import os
import gc
import warnings
import shutil
import hashlib
import numpy as np
import pandas as pd
import torch
from scipy.spatial.distance import pdist
from scipy.spatial import procrustes
from scipy.stats import spearmanr, pearsonr
from scipy.linalg import orthogonal_procrustes
from sklearn.metrics.pairwise import rbf_kernel
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from pathlib import Path

from huggingface_hub import login

# Authenticate with Hugging Face
token = os.environ.get("HF_TOKEN")
if token:
    login(token)
else:
    print("Set HF_TOKEN environment variable")

warnings.filterwarnings("ignore")

# Known causal LM model types for padding side detection
CAUSAL_TYPES = {
    "llama", "mistral", "falcon", "gpt2", "gpt_neo", "gpt_neox", "bloom", "opt",
    "qwen", "qwen2", "gemma", "gemma2", "phi", "stablelm", "mpt", "pythia",
    "tinyllama", "smollm", "starcoder", "codegen", "cohere", "dbrx"
}


def stable_seed(s: str, base: int = 0) -> int:
    """Generate a stable seed from a string, deterministic across Python runs."""
    h = hashlib.sha256(s.encode("utf-8")).digest()
    return (int.from_bytes(h[:8], "little") + base) % (2**32)

# --- RSATOOLBOX CHECK ---
try:
    import rsatoolbox
    from rsatoolbox.rdm.compare import compare as rsatoolbox_compare
    HAS_RSATOOLBOX = True
    print(">>> rsatoolbox available")
except ImportError:
    HAS_RSATOOLBOX = False
    print(">>> rsatoolbox not available")

# --- CONFIG ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RANDOM_STATE = 320

np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)
if DEVICE == "cuda":
    torch.cuda.manual_seed_all(RANDOM_STATE)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    DTYPE = torch.bfloat16
    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
    BATCH_SIZE = 128 if total_mem >= 70 else 64 if total_mem >= 35 else 32
    print(f"GPU: {total_mem:.1f}GB, Batch: {BATCH_SIZE}")
else:
    DTYPE = torch.float32
    BATCH_SIZE = 4


OUTPUT_DIR = Path("./shesha-drift")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


RESULTS_FILE = f"{OUTPUT_DIR}/drift_base_instruct_results.csv"

# Layer to extract embeddings from (-1 = last, -8 = 8th from end, etc.)
# Mid layers often have more stable geometry for non-embedding models
LAYER = -1  # Can change to -8 or -16 for deeper models

# Chat template settings
ADD_GENERATION_PROMPT = False  # False recommended for representation comparison (avoids assistant prefix bias)
INCLUDE_SYSTEM_MESSAGE = False  # Whether to include system message (can bias similarity upward)

# Bootstrap settings
N_BOOTSTRAP = 100

# Embedding settings
MAX_LENGTH = 256

# --- EXPANDED PROMPT SETS (50+ each for statistical power) ---
# These are semantically coherent sets to measure representation similarity
# Generated by LLMs

FACTUAL = [
    "The Earth orbits around the Sun.",
    "Water freezes at zero degrees Celsius.",
    "The human body has 206 bones.",
    "Light travels faster than sound.",
    "The Pacific Ocean is the largest ocean.",
    "Diamonds are made of carbon.",
    "The Moon causes tides on Earth.",
    "Oxygen is essential for human survival.",
    "The Great Wall of China spans thousands of miles.",
    "DNA contains genetic information.",
    "The Sahara is the largest hot desert.",
    "Mount Everest is the tallest mountain.",
    "The Amazon is the largest rainforest.",
    "Gold is a chemical element.",
    "The heart pumps blood through the body.",
    "Photosynthesis converts sunlight to energy.",
    "The brain controls the nervous system.",
    "Iron is magnetic.",
    "The Nile is the longest river.",
    "Gravity pulls objects toward Earth.",
    "The atmosphere protects Earth from meteors.",
    "Cells are the basic units of life.",
    "The speed of light is constant.",
    "Earthquakes occur along fault lines.",
    "The human eye can see millions of colors.",
    "Volcanoes release molten rock.",
    "The Earth has one natural satellite.",
    "Sound cannot travel through a vacuum.",
    "The periodic table organizes chemical elements.",
    "Hurricanes form over warm ocean water.",
    "The equator divides Earth into hemispheres.",
    "Mammals are warm-blooded animals.",
    "The ozone layer blocks ultraviolet radiation.",
    "Fossils preserve evidence of ancient life.",
    "The core of the Earth is extremely hot.",
    "Tectonic plates move slowly over time.",
    "The solar system has eight planets.",
    "Chlorophyll makes plants green.",
    "The human skeleton provides structure.",
    "Electricity flows through conductors.",
    "The Atlantic Ocean separates continents.",
    "Bacteria are single-celled organisms.",
    "The Sun is a star.",
    "Rainbows form from light refraction.",
    "The liver detoxifies the blood.",
    "Coral reefs support marine biodiversity.",
    "The Arctic is colder than the Antarctic.",
    "Muscles enable body movement.",
    "The universe is constantly expanding.",
    "Glaciers are made of compressed snow.",
]

DESCRIPTIVE = [
    "The old house stood quietly at the end of the street.",
    "Waves crashed against the rocky shoreline.",
    "The garden was filled with colorful flowers.",
    "A gentle breeze rustled through the leaves.",
    "The city lights sparkled in the distance.",
    "Snow covered the mountain peaks.",
    "The library was silent except for turning pages.",
    "Steam rose from the hot cup of coffee.",
    "The sunset painted the sky orange and pink.",
    "Children played happily in the park.",
    "The ancient castle overlooked the valley.",
    "Rain tapped softly against the window.",
    "The market was bustling with activity.",
    "A lone bird sang in the morning.",
    "The forest was dark and mysterious.",
    "Smoke curled up from the chimney.",
    "The river flowed peacefully through the meadow.",
    "Autumn leaves covered the ground.",
    "The bakery smelled of fresh bread.",
    "Stars filled the clear night sky.",
    "The dog slept peacefully by the fire.",
    "Fog rolled in from the sea.",
    "The train station was crowded with travelers.",
    "Sunlight streamed through the curtains.",
    "The beach stretched for miles.",
    "A cat watched birds from the window.",
    "The clock tower chimed at noon.",
    "Candles flickered in the dark room.",
    "The vineyard covered the hillside.",
    "Thunder rumbled in the distance.",
    "The caf√© was warm and inviting.",
    "Shadows lengthened as evening approached.",
    "The fountain splashed in the plaza.",
    "Ice crystals formed on the glass.",
    "The museum displayed ancient artifacts.",
    "Wildflowers bloomed along the path.",
    "The harbor was full of fishing boats.",
    "Morning dew glistened on the grass.",
    "The kitchen was filled with delicious aromas.",
    "Clouds drifted lazily across the sky.",
    "The bookshop was cozy and cluttered.",
    "Fireflies glowed in the summer night.",
    "The bridge spanned the wide river.",
    "Frost covered the windowpane.",
    "The orchard was heavy with ripe fruit.",
    "Waves lapped gently at the shore.",
    "The tower rose above the skyline.",
    "Petals fell from the cherry blossoms.",
    "The lantern cast a warm glow.",
    "Mountains framed the distant horizon.",
]

INSTRUCTIONS = [
    "Explain how photosynthesis works in plants.",
    "Describe the process of making bread from scratch.",
    "List the steps to change a car tire.",
    "Explain the water cycle in simple terms.",
    "Describe how to perform CPR correctly.",
    "Explain the difference between weather and climate.",
    "List the ingredients needed for a basic pasta sauce.",
    "Describe how earthquakes are measured.",
    "Explain the concept of supply and demand.",
    "Describe the steps to plant a vegetable garden.",
    "Explain how vaccines work in the body.",
    "List the major causes of air pollution.",
    "Describe the process of recycling plastic.",
    "Explain the difference between renewable and nonrenewable energy.",
    "Describe how to write a professional email.",
    "Explain the basic principles of electricity.",
    "List the benefits of regular exercise.",
    "Describe how the human digestive system works.",
    "Explain the concept of compound interest.",
    "Describe the steps to create a budget.",
    "Explain how GPS navigation works.",
    "List the symptoms of common cold versus flu.",
    "Describe the process of photographic development.",
    "Explain the difference between mitosis and meiosis.",
    "Describe how to tie a Windsor knot.",
    "Explain the greenhouse effect.",
    "List the main food groups for a balanced diet.",
    "Describe how sound waves travel.",
    "Explain the concept of natural selection.",
    "Describe the steps to start a small business.",
    "Explain how the immune system fights infection.",
    "List the planets in order from the Sun.",
    "Describe the process of making cheese.",
    "Explain the difference between acids and bases.",
    "Describe how to perform basic first aid.",
    "Explain the concept of inflation in economics.",
    "List the steps to prepare for a job interview.",
    "Describe how rainbows are formed.",
    "Explain the basics of machine learning.",
    "Describe the process of wine fermentation.",
    "Explain how the stock market works.",
    "List the causes and effects of deforestation.",
    "Describe how to maintain good mental health.",
    "Explain the theory of relativity simply.",
    "Describe the steps to learn a new language.",
    "Explain how batteries store energy.",
    "List the major events of World War II.",
    "Describe how to write a research paper.",
    "Explain the concept of biodiversity.",
    "Describe the process of cloud formation.",
]

CONVERSATIONAL = [
    "How was your day today?",
    "What do you think about the weather?",
    "Have you seen any good movies lately?",
    "What are your plans for the weekend?",
    "Do you have any book recommendations?",
    "What's your favorite type of music?",
    "Have you tried any new restaurants recently?",
    "What hobbies do you enjoy?",
    "Do you prefer coffee or tea?",
    "What's the best vacation you've ever had?",
    "How do you usually spend your evenings?",
    "What's your favorite season and why?",
    "Do you enjoy cooking at home?",
    "What sports do you follow?",
    "Have you learned anything interesting lately?",
    "What's your morning routine like?",
    "Do you prefer cities or countryside?",
    "What's the last thing that made you laugh?",
    "Do you have any pets?",
    "What's your favorite holiday tradition?",
    "How do you stay motivated?",
    "What's something you're looking forward to?",
    "Do you enjoy gardening?",
    "What's your favorite childhood memory?",
    "How do you like to relax after work?",
    "What's the best advice you've received?",
    "Do you prefer reading or watching movies?",
    "What's your favorite type of cuisine?",
    "Have you picked up any new skills recently?",
    "What's your ideal weekend activity?",
    "Do you enjoy outdoor activities?",
    "What's the most interesting place you've visited?",
    "How do you handle stress?",
    "What's your favorite thing about your job?",
    "Do you prefer mornings or evenings?",
    "What's something you'd like to learn?",
    "Have you attended any concerts lately?",
    "What's your favorite way to exercise?",
    "Do you enjoy board games or puzzles?",
    "What's your go-to comfort food?",
    "How do you stay organized?",
    "What's the best gift you've ever received?",
    "Do you have a favorite local spot?",
    "What's something that inspires you?",
    "How do you like to celebrate birthdays?",
    "What's your favorite thing to do on a rainy day?",
    "Do you enjoy traveling alone or with others?",
    "What's a skill you wish you had?",
    "How do you unwind before bed?",
    "What's something you're grateful for today?",
]

PROMPT_SETS = {
    'factual': FACTUAL,
    'descriptive': DESCRIPTIVE,
    'instructions': INSTRUCTIONS,
    'conversational': CONVERSATIONAL,
}

# --- CLEANUP UTILS ---
def clear_mem():
    gc.collect()
    if DEVICE == "cuda":
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def force_delete_model(model_id):
    """Delete model from HF cache with exact prefix matching."""
    try:
        cache_root = "/root/.cache/huggingface/hub"
        if not os.path.exists(cache_root):
            return

        safe_name = model_id.replace("/", "--")
        # Match exact prefixes only
        patterns = [
            f"models--{safe_name}",
            f"sentence-transformers--{safe_name}",
        ]

        for item in os.listdir(cache_root):
            for pattern in patterns:
                if item == pattern or item.startswith(pattern + "."):
                    path = os.path.join(cache_root, item)
                    if os.path.isdir(path):
                        shutil.rmtree(path)
                        print(f"   [Cleanup] Removed: {item}")
    except Exception as e:
        print(f"   [Cleanup Error] {model_id}: {e}")


def mem_info():
    if DEVICE == "cuda":
        return f"{torch.cuda.memory_allocated()/1e9:.1f}GB"
    return "CPU"


# --- L2 NORMALIZATION ---
def l2_normalize(X):
    """L2 normalize embeddings to unit norm."""
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    norms = np.maximum(norms, 1e-9)
    return X / norms


# --- METRICS ---
def cka_linear(X, Y):
    """
    Simple linear CKA (not debiased) for sanity checks.
    More numerically stable for self-similarity tests.
    """
    X = X - X.mean(axis=0, keepdims=True)
    Y = Y - Y.mean(axis=0, keepdims=True)

    num = np.linalg.norm(X.T @ Y, 'fro') ** 2
    den = np.linalg.norm(X.T @ X, 'fro') * np.linalg.norm(Y.T @ Y, 'fro')
    return float(num / (den + 1e-12))


def cka_debiased(X, Y):
    """Cleaner implementation of debiased CKA."""
    X = np.asarray(X, dtype=np.float64)
    Y = np.asarray(Y, dtype=np.float64)
    
    # Center the data
    X = X - X.mean(axis=0, keepdims=True)
    Y = Y - Y.mean(axis=0, keepdims=True)
    
    n = X.shape[0]
    if n < 4:
        # Fallback for tiny samples (standard CKA)
        num = np.linalg.norm(X.T @ Y, 'fro') ** 2
        den = np.linalg.norm(X.T @ X, 'fro') * np.linalg.norm(Y.T @ Y, 'fro')
        return float(num / (den + 1e-12))
    
    # Center kernel helper
    def center_gram_matrix(G):
        """Center a Gram matrix: H @ G @ H"""
        row_means = G.mean(axis=1, keepdims=True)
        col_means = G.mean(axis=0, keepdims=True)
        grand_mean = G.mean()
        return G - row_means - col_means + grand_mean
    
    # Compute and center Gram matrices
    K = center_gram_matrix(X @ X.T)
    L = center_gram_matrix(Y @ Y.T)
    
    # Zero diagonals for debiasing terms
    K_no_diag = K.copy()
    L_no_diag = L.copy()
    np.fill_diagonal(K_no_diag, 0)
    np.fill_diagonal(L_no_diag, 0)
    
    # Debiased HSIC estimator (Kornblith et al., 2019)
    hsic = (np.sum(K * L) 
            + (np.sum(K_no_diag) * np.sum(L_no_diag)) / ((n-1)*(n-2))
            - 2 * np.sum(np.sum(K_no_diag, axis=1) * np.sum(L_no_diag, axis=1)) / (n-2)
           ) / (n * (n-3))
    
    # Self-HSIC for normalization
    hsic_xx = (np.sum(K * K) 
               + np.sum(K_no_diag)**2 / ((n-1)*(n-2))
               - 2 * np.sum(np.sum(K_no_diag, axis=1)**2) / (n-2)
              ) / (n * (n-3))
    
    hsic_yy = (np.sum(L * L) 
               + np.sum(L_no_diag)**2 / ((n-1)*(n-2))
               - 2 * np.sum(np.sum(L_no_diag, axis=1)**2) / (n-2)
              ) / (n * (n-3))
    
    if hsic_xx <= 0 or hsic_yy <= 0:
        return 0.0
    
    return hsic / np.sqrt(hsic_xx * hsic_yy)


def rdm_spearman(X, Y):
    """RDM similarity via Spearman correlation of pairwise cosine distances."""
    rx = pdist(X, 'cosine')
    ry = pdist(Y, 'cosine')
    rho = spearmanr(rx, ry).correlation
    return float(rho) if np.isfinite(rho) else 0.0


def rdm_pearson(X, Y):
    """RDM similarity via Pearson correlation of pairwise cosine distances."""
    rx = pdist(X, 'cosine')
    ry = pdist(Y, 'cosine')
    r, _ = pearsonr(rx, ry)
    return float(r) if np.isfinite(r) else 0.0


def wuc_rsa(X, Y):
    """Whitened Unbiased Cosine RSA using rsatoolbox."""
    if not HAS_RSATOOLBOX:
        return np.nan
    try:
        dx = rsatoolbox.data.Dataset(X.astype(np.float64))
        dy = rsatoolbox.data.Dataset(Y.astype(np.float64))
        rx = rsatoolbox.rdm.calc_rdm(dx, method='euclidean')
        ry = rsatoolbox.rdm.calc_rdm(dy, method='euclidean')
        sim = rsatoolbox_compare(rx, ry, method='cosine_cov')
        return float(sim[0, 0])
    except Exception:
        return np.nan


def procrustes_similarity(X, Y):
    """Procrustes similarity using SciPy's procrustes with robust numerical checks."""
    try:
        # Convert to float64 for better numerical stability
        X = np.asarray(X, dtype=np.float64)
        Y = np.asarray(Y, dtype=np.float64)
        
        # Check for NaN/Inf values
        if np.any(np.isnan(X)) or np.any(np.isnan(Y)):
            print("      Warning: NaN values in procrustes input")
            return float('nan')
        if np.any(np.isinf(X)) or np.any(np.isinf(Y)):
            print("      Warning: Inf values in procrustes input")
            return float('nan')
        
        # Check for all-zero or constant columns
        X_std = X.std(axis=0)
        Y_std = Y.std(axis=0)
        if np.any(X_std < 1e-12) or np.any(Y_std < 1e-12):
            print("      Warning: Low variance columns in procrustes input")
            # Add small noise to break degeneracy
            rng = np.random.default_rng(42)
            X = X + rng.normal(0, 1e-8, X.shape)
            Y = Y + rng.normal(0, 1e-8, Y.shape)
        
        # Center the data
        X_mean = X.mean(axis=0)
        Y_mean = Y.mean(axis=0)
        X_centered = X - X_mean
        Y_centered = Y - Y_mean
        
        # Check if matrices are degenerate (all points identical after centering)
        X_norm = np.linalg.norm(X_centered, 'fro')
        Y_norm = np.linalg.norm(Y_centered, 'fro')
        
        if X_norm < 1e-12 or Y_norm < 1e-12:
            print("      Warning: Degenerate matrices in procrustes (norm too small)")
            return float('nan')
        
        # Scale to unit Frobenius norm
        X_scaled = X_centered / X_norm
        Y_scaled = Y_centered / Y_norm
        
        # Use try-except for SVD convergence issues
        try:
            # Use orthogonal_procrustes instead of scipy.procrustes for better stability
            R, scale = orthogonal_procrustes(X_scaled, Y_scaled)
        except np.linalg.LinAlgError as e:
            if "SVD did not converge" in str(e):
                print(f"      Warning: SVD did not converge, using fallback")
                # Fallback: compute similarity via correlation of principal angles
                Ux, _, Vx = np.linalg.svd(X_scaled, full_matrices=False)
                Uy, _, Vy = np.linalg.svd(Y_scaled, full_matrices=False)
                cos_angles = np.abs(np.diag(Ux.T @ Uy))
                return float(np.mean(cos_angles))
            else:
                raise
        
        # Compute distance and convert to similarity
        dist = np.linalg.norm(X_scaled @ R - Y_scaled, 'fro')
        
        # Theoretical maximum distance for unit-norm matrices is sqrt(2)
        similarity = 1 - dist / np.sqrt(2)
        
        # Ensure the result is valid
        similarity = np.clip(similarity, 0, 1)
        
        # Check if result is reasonable
        if not np.isfinite(similarity):
            print(f"      Warning: Non-finite procrustes similarity: {similarity}")
            return float('nan')
            
        return float(similarity)
        
    except ValueError as e:
        if "must contain >1 unique points" in str(e):
            print("      Warning: Degenerate case in procrustes (insufficient unique points)")
            return float('nan')
        print(f"      Warning: ValueError in procrustes: {e}")
        return float('nan')
    except np.linalg.LinAlgError as e:
        print(f"      Warning: LinAlgError in procrustes: {e}")
        return float('nan')
    except Exception as e:
        print(f"      Warning: Unexpected error in procrustes: {e}")
        return float('nan')


def sliced_wasserstein(X, Y, n_proj=100, seed=42):
    """Sliced Wasserstein distance with configurable seed."""
    rng = np.random.default_rng(seed)
    dirs = rng.standard_normal((X.shape[1], n_proj))
    dirs /= np.linalg.norm(dirs, axis=0)
    Xp = np.sort(X @ dirs, axis=0)
    Yp = np.sort(Y @ dirs, axis=0)
    return float(np.mean(np.abs(Xp - Yp)))


def mmd_rbf(X, Y, gamma=None, seed=42):
    """Corrected MMD implementation"""
    if gamma is None:
        combined = np.vstack([X, Y])
        if len(combined) > 500:
            rng = np.random.default_rng(seed)
            idx = rng.choice(len(combined), 500, replace=False)
            combined = combined[idx]
        median_dist = np.median(pdist(combined))
        gamma = 1.0 / (max(median_dist, 0.1) ** 2)

    Kxx = rbf_kernel(X, X, gamma)
    Kyy = rbf_kernel(Y, Y, gamma)
    Kxy = rbf_kernel(X, Y, gamma)

    np.fill_diagonal(Kxx, 0)
    np.fill_diagonal(Kyy, 0)

    n, m = len(X), len(Y)

    # CORRECTED: Always use n*m for cross term
    mmd_sq = (Kxx.sum() / (n * (n - 1)) + 
              Kyy.sum() / (m * (m - 1)) - 
              2 * Kxy.sum() / (n * m))
    
    return float(np.sqrt(max(0, mmd_sq)))


def compute_metrics(X, Y, seed=42):
    """Compute all representation similarity metrics with robust error handling."""
    try:
        # Ensure float64 for numerical stability
        X = X.astype(np.float64)
        Y = Y.astype(np.float64)
        
        # Basic sanity checks
        if X.shape[0] < 2 or Y.shape[0] < 2:
            print("      Warning: Too few samples for metrics")
            return {metric: float('nan') for metric in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'mmd', 'wasserstein']}
        
        if np.any(np.isnan(X)) or np.any(np.isnan(Y)):
            print("      Warning: NaN values in input matrices")
            return {metric: float('nan') for metric in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'mmd', 'wasserstein']}
        
        if np.any(np.isinf(X)) or np.any(np.isinf(Y)):
            print("      Warning: Inf values in input matrices")
            return {metric: float('nan') for metric in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'mmd', 'wasserstein']}
        
        metrics = {}
        
        # Try each metric with individual error handling
        try:
            metrics['cka_debiased'] = cka_debiased(X, Y)
        except Exception as e:
            print(f"      Warning: cka_debiased failed: {e}")
            metrics['cka_debiased'] = float('nan')
        
        try:
            metrics['rdm_spearman'] = rdm_spearman(X, Y)
        except Exception as e:
            print(f"      Warning: rdm_spearman failed: {e}")
            metrics['rdm_spearman'] = float('nan')
        
        try:
            metrics['rdm_pearson'] = rdm_pearson(X, Y)
        except Exception as e:
            print(f"      Warning: rdm_pearson failed: {e}")
            metrics['rdm_pearson'] = float('nan')
        
        try:
            metrics['procrustes'] = procrustes_similarity(X, Y)
        except Exception as e:
            print(f"      Warning: procrustes failed: {e}")
            metrics['procrustes'] = float('nan')
        
        try:
            metrics['mmd'] = mmd_rbf(X, Y, seed=seed)
        except Exception as e:
            print(f"      Warning: mmd failed: {e}")
            metrics['mmd'] = float('nan')
        
        try:
            metrics['wasserstein'] = sliced_wasserstein(X, Y, seed=seed)
        except Exception as e:
            print(f"      Warning: wasserstein failed: {e}")
            metrics['wasserstein'] = float('nan')

        if HAS_RSATOOLBOX:
            try:
                metrics['wuc'] = wuc_rsa(X, Y)
            except Exception as e:
                print(f"      Warning: wuc failed: {e}")
                metrics['wuc'] = float('nan')

        return metrics
        
    except Exception as e:
        print(f"      Error in compute_metrics: {e}")
        return {metric: float('nan') for metric in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'mmd', 'wasserstein'] + (['wuc'] if HAS_RSATOOLBOX else [])}


def compute_metrics_with_bootstrap(X, Y, n_bootstrap=None, ci=0.95, seed=None):
    """
    Compute metrics with bootstrap confidence intervals.
    Returns dict with 'metric': value, 'metric_ci_lo': lower, 'metric_ci_hi': upper.
    """
    if n_bootstrap is None:
        n_bootstrap = N_BOOTSTRAP
    if seed is None:
        seed = RANDOM_STATE

    X = X.astype(np.float64)
    Y = Y.astype(np.float64)
    n = len(X)

    # Point estimates
    point_metrics = compute_metrics(X, Y, seed=seed)

    # Bootstrap for key metrics only (expensive)
    bootstrap_metrics = ['rdm_spearman', 'cka_debiased', 'procrustes']

    rng = np.random.default_rng(seed)
    bootstrap_samples = {m: [] for m in bootstrap_metrics}

    for _ in range(n_bootstrap):
        idx = rng.choice(n, size=n, replace=True)
        Xb, Yb = X[idx], Y[idx]

        for metric in bootstrap_metrics:
            if metric == 'rdm_spearman':
                val = rdm_spearman(Xb, Yb)
            elif metric == 'cka_debiased':
                val = cka_debiased(Xb, Yb)
            elif metric == 'procrustes':
                val = procrustes_similarity(Xb, Yb)
            else:
                continue
            bootstrap_samples[metric].append(val)

    # Compute CIs
    alpha = (1 - ci) / 2
    result = {}
    for metric, value in point_metrics.items():
        result[metric] = value
        if metric in bootstrap_samples and bootstrap_samples[metric]:
            samples = np.array(bootstrap_samples[metric])
            result[f"{metric}_ci_lo"] = float(np.percentile(samples, alpha * 100))
            result[f"{metric}_ci_hi"] = float(np.percentile(samples, (1 - alpha) * 100))

    return result


# --- EMBEDDING ---
def load_model_and_tokenizer(model_name):
    """
    Load model and tokenizer, setting appropriate padding side.
    Note: AutoModelForCausalLM fallback happens in embed_texts if hidden states fail.
    """
    print(f"   Loading {model_name} [{mem_info()}]")

    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            use_fast=True,
            trust_remote_code=True
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # Try AutoModel first
        model = AutoModel.from_pretrained(
            model_name,
            torch_dtype=DTYPE,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        ).to(DEVICE).eval()

        # Determine model type for padding side
        is_encoder_decoder = getattr(model.config, "is_encoder_decoder", False)
        is_decoder = getattr(model.config, "is_decoder", False)
        model_type = getattr(model.config, "model_type", "").lower()

        # Simple, reliable padding side logic
        if is_encoder_decoder:
            tokenizer.padding_side = "right"
            model_category = "encoder-decoder"
        elif is_decoder or (model_type in CAUSAL_TYPES):
            tokenizer.padding_side = "left"
            model_category = "causal-lm"
        else:
            tokenizer.padding_side = "right"
            model_category = "encoder-only"

        # Check if chat template exists
        has_chat_template = (
            hasattr(tokenizer, 'chat_template') and
            tokenizer.chat_template is not None
        )

        print(f"      model_type={model_type}, category={model_category}, padding={tokenizer.padding_side}, has_template={has_chat_template}")

        return model, tokenizer, has_chat_template
    except Exception as e:
        print(f"   Error loading {model_name}: {e}")
        return None, None, False


def reload_as_causal_lm(model_name):
    """Reload model using AutoModelForCausalLM if AutoModel failed to return hidden states."""
    print(f"      Retrying with AutoModelForCausalLM...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=DTYPE,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        ).to(DEVICE).eval()
        return model
    except Exception as e:
        print(f"      AutoModelForCausalLM also failed: {e}")
        return None


def apply_chat_template(tokenizer, texts):
    """
    Apply chat template if tokenizer has one defined.
    Does not rely on model name heuristics - just checks if template exists.
    Returns (processed_texts, template_was_used).
    """
    # Check if tokenizer has a chat template
    has_template = (
        hasattr(tokenizer, 'chat_template') and
        tokenizer.chat_template is not None
    )

    if not has_template:
        return texts, False

    try:
        templated = []
        for text in texts:
            if INCLUDE_SYSTEM_MESSAGE:
                messages = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": text}
                ]
            else:
                messages = [{"role": "user", "content": text}]

            try:
                formatted = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=ADD_GENERATION_PROMPT
                )
            except Exception:
                # Fallback: try without system message if it failed
                messages = [{"role": "user", "content": text}]
                formatted = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=ADD_GENERATION_PROMPT
                )
            templated.append(formatted)
        return templated, True
    except Exception as e:
        print(f"      Warning: Chat template failed ({e}), using raw text")
        return texts, False


def embed_texts(model, tokenizer, texts, model_name=None):
    """
    Embed texts using mean pooling of specified hidden layer, L2 normalized.
    Uses LAYER parameter to select which layer to pool from.
    Returns (embeddings, template_applied_successfully, model_was_reloaded).
    If hidden states not returned and model_name provided, will try AutoModelForCausalLM.
    """
    # Apply chat template if available
    processed_texts, template_applied = apply_chat_template(tokenizer, texts)
    if template_applied:
        print(f"      Applied chat template (add_gen_prompt={ADD_GENERATION_PROMPT}, sys_msg={INCLUDE_SYSTEM_MESSAGE})")

    all_vecs = []
    at_max_length_count = 0
    model_reloaded = False
    current_model = model

    for i in range(0, len(processed_texts), BATCH_SIZE):
        batch = processed_texts[i:i+BATCH_SIZE]
        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_LENGTH
        ).to(DEVICE)

        # Count sequences at max length
        seq_lengths = inputs["attention_mask"].sum(dim=1)
        at_max_length_count += int((seq_lengths == MAX_LENGTH).sum().item())

        with torch.no_grad():
            if DEVICE == "cuda":
                with torch.cuda.amp.autocast(enabled=True, dtype=DTYPE):
                    out = current_model(**inputs, output_hidden_states=True, return_dict=True)
            else:
                out = current_model(**inputs, output_hidden_states=True, return_dict=True)

            # Check that hidden_states was actually returned
            if not hasattr(out, "hidden_states") or out.hidden_states is None:
                if model_name is not None and not model_reloaded:
                    # Try reloading with AutoModelForCausalLM
                    new_model = reload_as_causal_lm(model_name)
                    if new_model is not None:
                        current_model = new_model
                        model_reloaded = True
                        clear_mem()
                        # Retry this batch
                        if DEVICE == "cuda":
                            with torch.cuda.amp.autocast(enabled=True, dtype=DTYPE):
                                out = current_model(**inputs, output_hidden_states=True, return_dict=True)
                        else:
                            out = current_model(**inputs, output_hidden_states=True, return_dict=True)

                        if not hasattr(out, "hidden_states") or out.hidden_states is None:
                            raise RuntimeError(f"Model still did not return hidden_states after reload.")
                    else:
                        raise RuntimeError(f"Model did not return hidden_states and AutoModelForCausalLM failed.")
                else:
                    raise RuntimeError(f"Model did not return hidden_states.")

            # Select layer
            hidden_states = out.hidden_states
            h = hidden_states[LAYER]

            # Mean pooling over non-padding tokens
            mask = inputs["attention_mask"].unsqueeze(-1)
            vecs = (h * mask).sum(1) / mask.sum(1).clamp(min=1e-9)
            all_vecs.append(vecs.float().cpu().numpy())

        del inputs, out, hidden_states, h, vecs

    if at_max_length_count > 0:
        padding_note = " (may indicate truncation)" if tokenizer.padding_side == "right" else ""
        print(f"      Info: {at_max_length_count}/{len(texts)} sequences at max_length={MAX_LENGTH}{padding_note}")

    embeddings = np.vstack(all_vecs)
    embeddings = l2_normalize(embeddings)

    # Return the potentially reloaded model if caller needs it
    return embeddings, template_applied, current_model if model_reloaded else None


def embed_all_prompt_sets(model, tokenizer, model_name):
    """
    Embed all prompt sets with a single model load.
    Returns (embeddings_dict, template_used, possibly_new_model, was_reloaded).
    """
    embeddings = {}
    template_used = False
    current_model = model
    any_reload = False

    for set_name, prompts in PROMPT_SETS.items():
        emb, used, new_model = embed_texts(current_model, tokenizer, prompts, model_name)
        if new_model is not None:
            # Clean up old model before switching
            old_model = current_model
            current_model = new_model
            any_reload = True
            del old_model
            clear_mem()
        embeddings[set_name] = emb
        template_used = template_used or used
        print(f"      {set_name}: {emb.shape}")

    return embeddings, template_used, current_model, any_reload


# --- MODELS ---
PAIRS = [
    # Small models
    ("HuggingFaceTB/SmolLM-135M", "HuggingFaceTB/SmolLM-135M-Instruct", 0.135),
    ("HuggingFaceTB/SmolLM2-135M", "HuggingFaceTB/SmolLM2-135M-Instruct", 0.135),
    ("HuggingFaceTB/SmolLM-360M", "HuggingFaceTB/SmolLM-360M-Instruct", 0.36),
    ("HuggingFaceTB/SmolLM2-360M", "HuggingFaceTB/SmolLM2-360M-Instruct", 0.36),
    ("Qwen/Qwen2-0.5B", "Qwen/Qwen2-0.5B-Instruct", 0.5),
    ("Qwen/Qwen1.5-0.5B", "Qwen/Qwen1.5-0.5B-Chat", 0.5),
    ("bigscience/bloom-560m", "bigscience/bloomz-560m", 0.56),
    # 1B models
    ("EleutherAI/pythia-1b", "EleutherAI/pythia-1b-deduped", 1.0),
    ("meta-llama/Llama-3.2-1B", "meta-llama/Llama-3.2-1B-Instruct", 1.0),
    ("TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T", "TinyLlama/TinyLlama-1.1B-Chat-v1.0", 1.1),
    ("bigscience/bloom-1b1", "bigscience/bloomz-1b1", 1.1),
    ("Qwen/Qwen2-1.5B", "Qwen/Qwen2-1.5B-Instruct", 1.5),
    ("stabilityai/stablelm-2-1_6b", "stabilityai/stablelm-2-zephyr-1_6b", 1.6),
    ("HuggingFaceTB/SmolLM-1.7B", "HuggingFaceTB/SmolLM-1.7B-Instruct", 1.7),
    ("HuggingFaceTB/SmolLM2-1.7B", "HuggingFaceTB/SmolLM2-1.7B-Instruct", 1.7),
    ("Qwen/Qwen1.5-1.8B", "Qwen/Qwen1.5-1.8B-Chat", 1.8),
    # 2-4B models
    ("google/gemma-2b", "google/gemma-2b-it", 2.0),
    ("google/gemma-2-2b", "google/gemma-2-2b-it", 2.0),
    ("meta-llama/Llama-3.2-3B", "meta-llama/Llama-3.2-3B-Instruct", 3.0),
    ("Qwen/Qwen1.5-4B", "Qwen/Qwen1.5-4B-Chat", 4.0),
    # 7B models
    ("Qwen/Qwen2-7B", "Qwen/Qwen2-7B-Instruct", 7.0),
    ("mistralai/Mistral-7B-v0.1", "mistralai/Mistral-7B-Instruct-v0.1", 7.0),
    ("tiiuae/falcon-7b", "tiiuae/falcon-7b-instruct", 7.0),
    ("meta-llama/Llama-2-7b-hf", "meta-llama/Llama-2-7b-chat-hf", 7.0),
]


# --- MAIN ---
def run():
    # Resume check
    all_results = []
    completed_pairs = set()

    if os.path.exists(RESULTS_FILE):
        try:
            df_exist = pd.read_csv(RESULTS_FILE)
            completed_pairs = set(df_exist['pair'].unique())
            all_results = df_exist.to_dict('records')
            print(f"Resuming... {len(completed_pairs)} pairs done, {len(all_results)} rows loaded.")
        except Exception as e:
            print(f"Warning: Could not load existing results: {e}")

    print(f"Total pairs: {len(PAIRS)}, rsatoolbox: {HAS_RSATOOLBOX}")
    print(f"Prompts per set: {len(FACTUAL)}")

    for i, (base_name, tuned_name, size) in enumerate(PAIRS):
        pair_key = f"{base_name}->{tuned_name}"

        if pair_key in completed_pairs:
            print(f"\n[Skip] {pair_key}")
            continue

        print(f"\n{'='*60}")
        print(f"[{i+1}/{len(PAIRS)}] {pair_key} ({size}B)")
        print(f"{'='*60}")

        # Skip large models on small GPUs
        if DEVICE == "cuda" and size >= 7:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
            if gpu_mem < 35:
                print("   Skipping (GPU too small for 7B)")
                continue

        # --- Load and embed BASE model ---
        print(f"   [BASE] {base_name}")
        base_model, base_tokenizer, base_has_template = load_model_and_tokenizer(base_name)
        if base_model is None:
            continue

        try:
            base_embeddings, base_template_applied, base_model, base_reloaded = embed_all_prompt_sets(base_model, base_tokenizer, base_name)
        except RuntimeError as e:
            print(f"   Error embedding base: {e}")
            del base_model, base_tokenizer
            clear_mem()
            force_delete_model(base_name)
            continue

        # Free base model immediately
        del base_model, base_tokenizer
        clear_mem()
        force_delete_model(base_name)

        # --- Load and embed TUNED model ---
        print(f"   [TUNED] {tuned_name}")
        tuned_model, tuned_tokenizer, tuned_has_template = load_model_and_tokenizer(tuned_name)
        if tuned_model is None:
            del base_embeddings
            clear_mem()
            continue

        try:
            tuned_embeddings, tuned_template_applied, tuned_model, tuned_reloaded = embed_all_prompt_sets(tuned_model, tuned_tokenizer, tuned_name)
        except RuntimeError as e:
            print(f"   Error embedding tuned: {e}")
            del tuned_model, tuned_tokenizer, base_embeddings
            clear_mem()
            force_delete_model(tuned_name)
            continue

        # Free tuned model
        del tuned_model, tuned_tokenizer
        clear_mem()
        force_delete_model(tuned_name)

        # --- Compute metrics ---
        print("   Computing metrics...")
        record = {
            "pair": pair_key,
            "size": size,
            "layer": LAYER,
            "max_length": MAX_LENGTH,
            "n_bootstrap": N_BOOTSTRAP,
            "add_generation_prompt": ADD_GENERATION_PROMPT,
            "include_system_message": INCLUDE_SYSTEM_MESSAGE,
            "base_has_template": base_has_template,
            "base_template_applied": base_template_applied,
            "base_reloaded_causal_lm": base_reloaded,
            "tuned_has_template": tuned_has_template,
            "tuned_template_applied": tuned_template_applied,
            "tuned_reloaded_causal_lm": tuned_reloaded,
        }

        # Stable RNG seed per pair for reproducibility across Python runs
        pair_seed = stable_seed(pair_key, RANDOM_STATE)

        record["pair_seed"] = pair_seed
        record["bootstrap_seed_offset"] = 17
        record["sanity_seed_offset"] = 999

        for i_set, set_name in enumerate(PROMPT_SETS):
            X = base_embeddings[set_name]
            Y = tuned_embeddings[set_name]

            # Add this check
            if X.shape[0] < 2 or Y.shape[0] < 2:
                print(f"      Warning: {set_name} has insufficient samples ({X.shape[0]}, {Y.shape[0]})")
                continue  # Skip this prompt set

            # Comprehensive sanity checks on first prompt set
            if set_name == 'factual':
                # Seed for sanity checks
                sanity_seed = pair_seed + 999

                # Self-similarity using linear CKA (more stable for self-check)
                self_rdm = rdm_spearman(X, X)
                self_cka_linear = cka_linear(X, X)
                self_proc = procrustes_similarity(X, X)
                self_wass = sliced_wasserstein(X, X, seed=sanity_seed)
                self_mmd = mmd_rbf(X, X, seed=sanity_seed)
                print(f"      Self-similarity sanity:")
                print(f"        rdm_spearman={self_rdm:.4f} (expect ~1)")
                print(f"        cka_linear={self_cka_linear:.4f} (expect ~1)")
                print(f"        procrustes={self_proc:.4f} (expect ~1)")
                print(f"        wasserstein={self_wass:.4f} (expect ~0)")
                print(f"        mmd={self_mmd:.4f} (expect ~0)")

                if self_rdm < 0.99 or self_cka_linear < 0.99:
                    print("      WARNING: Self-similarity sanity check failed!")

                # Permutation null with sign flip for stronger stress test
                rng = np.random.default_rng(pair_seed)
                perm = rng.permutation(len(X))
                sign = rng.choice([-1, 1], size=(1, X.shape[1]))
                perm_rdm = rdm_spearman(X, X[perm])
                perm_cka = cka_linear(X, X[perm] * sign)  # Permutation + sign flip stress test
                print(f"      Permutation null (+ sign flip for CKA):")
                print(f"        rdm_spearman={perm_rdm:.4f} (expect << 1)")
                print(f"        cka_linear={perm_cka:.4f} (expect << 1)")

                if perm_rdm > 0.2 or perm_cka > 0.2:
                    print("      WARNING: Permutation null unexpectedly high!")

                # Cross-set null: factual vs descriptive (permuted for CKA)
                X_desc = base_embeddings['descriptive']
                perm_desc = rng.permutation(len(X_desc))
                cross_rdm = rdm_spearman(X, X_desc)  # RDM is permutation-invariant
                cross_cka = cka_linear(X, X_desc[perm_desc])  # Permute to avoid index artifacts
                print(f"      Cross-set null (factual vs descriptive, same model):")
                print(f"        rdm_spearman={cross_rdm:.4f}, cka_linear={cross_cka:.4f}")
                record["cross_set_rdm_spearman"] = cross_rdm
                record["cross_set_cka_linear"] = cross_cka

            # Compute metrics with bootstrap CIs using per-pair-set seed
            bootstrap_seed = pair_seed + 17 + i_set
            metrics = compute_metrics_with_bootstrap(X, Y, seed=bootstrap_seed)

            for metric_name, value in metrics.items():
                record[f"{metric_name}_{set_name}"] = value
                # Store dissimilarity for main metrics (not CIs)
                if metric_name in ['cka_debiased', 'rdm_spearman', 'rdm_pearson', 'procrustes', 'wuc']:
                    record[f"dissim_{metric_name}_{set_name}"] = 1 - value if np.isfinite(value) else np.nan

        all_results.append(record)

        # Log summary
        rdm_s = record.get('rdm_spearman_factual', 0)
        cka = record.get('cka_debiased_factual', 0)
        print(f"   factual: rdm_spearman={rdm_s:.3f}, cka={cka:.3f}")

        # Clean up embeddings
        del base_embeddings, tuned_embeddings
        clear_mem()

        # Save after each pair
        completed_pairs.add(pair_key)
        pd.DataFrame(all_results).to_csv(RESULTS_FILE, index=False)
        print(f"   Saved ({len(all_results)} rows)")

    # Final summary
    print(f"\n{'='*60}")
    print(f"DONE - Saved to {RESULTS_FILE}")
    print(f"{'='*60}")

    if all_results:
        df = pd.DataFrame(all_results)
        print(f"\nTotal rows: {len(df)}")
        print(f"Pairs completed: {df['pair'].nunique()}")

        print("\n--- MEAN SIMILARITY ACROSS PROMPT SETS ---")
        for metric in ['rdm_spearman', 'cka_debiased', 'procrustes', 'wuc']:
            cols = [c for c in df.columns if c.startswith(f"{metric}_") and not c.startswith("dissim_")]
            if cols:
                mean_val = df[cols].mean().mean()
                print(f"  {metric}: {mean_val:.4f}")


if __name__ == "__main__":
    run()